{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "collected-skiing",
   "metadata": {},
   "source": [
    "# Minimal Risk Decomposition Code\n",
    "\n",
    "This notebook contains a minimal pipeline for computing the risk decomposition from [...]. The focus is on simplicity and understandibility.\n",
    "\n",
    "**Make sure that you use a GPU** (on COLAB: runtime -> change runtime type -> Hardware accelerator: GPU)\n",
    "\n",
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ideal-onion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.8 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision tqdm pytorch-lightning pandas sklearn git+https://github.com/openai/CLIP.git --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-yield",
   "metadata": {},
   "source": [
    "## Pretrain\n",
    "First we will download the desired pretrained model. The following command returns the compressor as well as the transform that should be applied to the images before compression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "liberal-aspect",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/YannDubs_SSL-Risk-Decomposition_main\n",
      "Using cache found in /root/.cache/torch/hub/facebookresearch_dino_main\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Using cache found in /root/.cache/torch/hub/YannDubs_SSL-Risk-Decomposition_main\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# loads the desired pretrained model and preprocessing pipeline\n",
    "name = \"dino_rn50\" # example\n",
    "model, preprocessor = torch.hub.load('YannDubs/SSL-Risk-Decomposition:main', name, trust_repo=True)\n",
    "\n",
    "# loads all results and hyperparameters\n",
    "results_df = torch.hub.load('YannDubs/SSL-Risk-Decomposition:main', \"results_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "amazing-australian",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import Food101\n",
    "DATA_DIR = \"data/\"\n",
    "\n",
    "# Load some data to compute the SSL risk decomposition. \n",
    "# This should be the data on which the model was pretrained (ie ImageNet) but requires downloading => let's use Food as an example\n",
    "train = Food101(DATA_DIR, download=True, split=\"train\", transform=preprocessor)\n",
    "test = Food101(DATA_DIR, download=True, split=\"test\", transform=preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-photographer",
   "metadata": {},
   "source": [
    "## Computing the SSL risk decomposition \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "affiliated-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_risk_components(model_ssl, data_train, data_test, model_sup=None, n_sub=10000, **kwargs):\n",
    "    \"\"\"Computes the SSL risk decomposition for `model_ssl` using a given training and testing set.\n",
    "    \n",
    "    If we are given a supervised `model_sup` of the same architecture as model_ssl, we compute the \n",
    "    approximation error. Else we merge it with usability error given that approx error is neglectable.\n",
    "    \"\"\"\n",
    "    errors = dict()\n",
    "    \n",
    "    # featurize data to make probing much faster. Optional.\n",
    "    D_train = featurize_data(model_ssl, data_train)\n",
    "    D_test = featurize_data(model_ssl, data_test)\n",
    "    \n",
    "    D_comp, D_sub = data_split(D_train, n=n_sub)\n",
    "    \n",
    "    r_A_F = train_eval_probe(D_train, D_train, **kwargs)\n",
    "    r_A_S = train_eval_probe(D_comp, D_sub, **kwargs)\n",
    "    r_U_S = train_eval_probe(D_train, D_test, **kwargs)\n",
    "    \n",
    "    if model_sup is not None:\n",
    "        D_train_sup = featurize_data(model_sup, data_train)\n",
    "        errors[\"approx\"] = train_eval_probe(D_train_sup, D_train_sup, **kwargs)\n",
    "        errors[\"usability\"] = r_A_F - errors[\"approx\"]\n",
    "    else:\n",
    "        errors[\"usability\"] = r_A_F # merges both errors but approx is neglectable\n",
    "        \n",
    "    errors[\"probe_gen\"] = r_A_S - r_A_F\n",
    "    errors[\"encoder_gen\"] = r_U_S - r_A_S \n",
    "    errors[\"agg_risk\"] = r_U_S\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-survey",
   "metadata": {},
   "source": [
    "The above function is the general risk decomposition that is agnostic to the specific implementation of the the linear probing and data.\n",
    "Below we give a specific implementation using Pytorch. Those functions should can easily be modified for different choices (eg to use sklearn or tune the probe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "elder-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "def featurize_data(model, dataset):\n",
    "    \"\"\"Featurize a dataset using the model.\"\"\"\n",
    "    model = model.eval().cuda().half()\n",
    "    with torch.no_grad():\n",
    "        Z, Y = [], []\n",
    "        for x, y in tqdm.tqdm(DataLoader(dataset, batch_size=512, num_workers=8)):\n",
    "            Z += [model(x.to(\"cuda\").half()).cpu().numpy()]\n",
    "            Y += [y.cpu().numpy()]\n",
    "    return SklearnDataset(np.concatenate(Z), np.concatenate(Y))\n",
    "\n",
    "\n",
    "def train_eval_probe(D_train, D_test, max_epochs=100, batch_size=4096, n_workers=os.cpu_count(), lr=1e-3):\n",
    "    \"\"\"Trains a model (encoder and probe) on D_train and evaluates it on D_test\"\"\"\n",
    "    probe = LogisticRegression(in_dim=len(D_train[0][0]), out_dim=len(train.classes), \n",
    "                               max_epochs=max_epochs, lr=batch_size/256*lr)\n",
    "    loader_train = DataLoader(D_train, batch_size=batch_size, shuffle=True, \n",
    "                              num_workers=n_workers, pin_memory=True)\n",
    "    trainer = pl.Trainer(logger=False, accelerator=\"auto\", enable_checkpointing=False, \n",
    "                         max_epochs=max_epochs, precision=16)\n",
    "    trainer.fit(probe, train_dataloaders=loader_train)\n",
    "    \n",
    "    loader_test = DataLoader(D_test, batch_size=batch_size*2, shuffle=False, \n",
    "                             num_workers=n_workers, pin_memory=True)\n",
    "    logs = trainer.test(dataloaders=loader_test, ckpt_path=None, model=probe)[0]\n",
    "    return logs[\"err\"]\n",
    "\n",
    "def data_split(D, n, seed=123):\n",
    "    \"\"\"Split a dataset into a set of size n and its complement\"\"\"\n",
    "    complement_idcs, subset_idcs = train_test_split(range(len(D)), stratify=D.Y, test_size=n, random_state=seed)\n",
    "    return Subset(D, indices=complement_idcs), Subset(D, indices=subset_idcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "consistent-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        super().__init__()\n",
    "        self.X, self.Y = X.astype(np.float32), y.astype(np.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "\n",
    "class LogisticRegression(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, max_epochs=100, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.probe = torch.nn.Linear(in_dim, out_dim)\n",
    "        self.max_epochs = max_epochs\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.probe(x)\n",
    "\n",
    "    def step(self, batch):\n",
    "        x, y = batch\n",
    "        Y_hat = self(x)\n",
    "        acc = (Y_hat.argmax(dim=-1) == y).sum() / y.shape[0] \n",
    "        self.log(f\"err\", 1-acc, prog_bar=True)\n",
    "        return F.cross_entropy(Y_hat, y.squeeze().long())\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.step(batch)\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self.step(batch)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.probe.parameters(), lr=self.lr, weight_decay=1e-7)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, self.max_epochs)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "processed-bundle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 148/148 [02:26<00:00,  1.01it/s]\n",
      "100%|██████████| 50/50 [00:54<00:00,  1.08s/it]\n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | probe | Linear | 206 K \n",
      "---------------------------------\n",
      "206 K     Trainable params\n",
      "0         Non-trainable params\n",
      "206 K     Total params\n",
      "0.414     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc00390d537e4c6a89153bb24ddc784d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0695487f87084344a00116049b27436b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "           err              0.02899010293185711\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | probe | Linear | 206 K \n",
      "---------------------------------\n",
      "206 K     Trainable params\n",
      "0         Non-trainable params\n",
      "206 K     Total params\n",
      "0.414     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38df0388bca246b2998ce1d623ae84df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9f90748d1143a08d61e11fcafda15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "           err              0.2696000039577484\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type   | Params\n",
      "---------------------------------\n",
      "0 | probe | Linear | 206 K \n",
      "---------------------------------\n",
      "206 K     Trainable params\n",
      "0         Non-trainable params\n",
      "206 K     Total params\n",
      "0.414     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f89b5d811c7463bad127e8e3215ec44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888a62b9ef924e009cf86c4c12474de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "           err              0.22605940699577332\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "errors = compute_risk_components(model, train, test, max_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "directed-auditor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "usability       2.899010\n",
       "probe_gen      24.060990\n",
       "encoder_gen    -4.354060\n",
       "agg_risk       22.605941\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(errors) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-potential",
   "metadata": {},
   "source": [
    "Note that the estimate of encoder generalization here is not meaningfull because the model was not pretrained on the dataset we are using.\n",
    "Despite this issue we see that the risk components are surprisingly similar given that we used a different dataset for computing and did not hyperparameter tune the probe. Results for ImageNet and tuned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "polish-stake",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agg_risk     25.828001\n",
       "approx        0.845089\n",
       "enc_gen          3.336\n",
       "probe_gen    21.420243\n",
       "usability     0.226668\n",
       "Name: dino_rn50, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.loc[name, \"risk_decomposition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-jumping",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
