{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0a51357-237e-494d-8dfa-2f66a3ad29c5",
   "metadata": {},
   "source": [
    "# Minimal Risk Decomposition Code\n",
    "\n",
    "This notebook contains a minimal pipeline for computing the risk decomposition from [...]. The focus is on simplicity and understandibility.\n",
    "\n",
    "**Make sure that you use a GPU** (on COLAB: runtime -> change runtime type -> Hardware accelerator: GPU)\n",
    "\n",
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf747146-694c-4ab4-bdec-30b65f2f583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision tqdm pytorch-lightning pandas sklearn git+https://github.com/openai/CLIP.git --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a98e0b-a3ee-45c6-ad4b-3c966a00aba8",
   "metadata": {},
   "source": [
    "## Standard: model and data\n",
    "First we will download the desired pretrained model. The following command returns the compressor as well as the transform that should be applied to the images before compression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c779b13b-98df-4078-9bc5-d616542e2b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/yanndubois/.cache/torch/hub/YannDubs_SSL-Risk-Decomposition_main\n",
      "Using cache found in /Users/yanndubois/.cache/torch/hub/YannDubs_SSL-Risk-Decomposition_main\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# loads the desired pretrained model and preprocessing pipeline\n",
    "name = \"clip_rn50\" # example\n",
    "model, preprocessor = torch.hub.load('YannDubs/SSL-Risk-Decomposition:main', name, trust_repo=True)\n",
    "\n",
    "# loads all results and hyperparameters\n",
    "results_df = torch.hub.load('YannDubs/SSL-Risk-Decomposition:main', \"results_df\")\n",
    "z_dim = results_df.hyperparameters.z_dim[name.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81f86dd4-378d-475c-9aa5-0e7dcb90ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import Food101\n",
    "DATA_DIR = \"data/\"\n",
    "\n",
    "# Load some data to compress and apply transformation\n",
    "train = Food101(DATA_DIR, download=True, split=\"train\", transform=preprocessor)\n",
    "test = Food101(DATA_DIR, download=True, split=\"test\", transform=preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7515a08d-0128-4819-9bef-7e7cff296558",
   "metadata": {},
   "source": [
    "## Computing the SSL risk decomposition \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e73237f8-3c4c-4b85-b45d-666f25168b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_risk_components(model_ssl, D_train, D_test, model_sup=None):\n",
    "    \"\"\"Computes the SSL risk decomposition for `model_ssl` using a given training and testing set.\n",
    "    \n",
    "    If we are given a supervised `model_sup` of the same architecture as model_ssl, we compute the \n",
    "    approximation error. Else we merge it with usability error given that approx error is neglectable.\n",
    "    \"\"\"\n",
    "    errors = dict()\n",
    "        \n",
    "    D_comp, D_sub = data_split(D_train, n=len(D_test))\n",
    "    \n",
    "    r_A_F = train_eval_model(model_ssl, D_train, D_train)\n",
    "    r_A_S = train_eval_model(model_ssl, D_comp, D_sub)\n",
    "    r_U_S = train_eval_model(model_ssl, D_train, D_test)\n",
    "    \n",
    "    if model_sup is not None:\n",
    "        errors[\"approx\"] = eval_model(model_sup, D_train)\n",
    "        errors[\"usability\"] = r_A_F - out[\"approx\"]\n",
    "    else:\n",
    "        errors[\"usability\"] = r_A_F # merges both errors but approx is neglectable\n",
    "        \n",
    "    out[\"probe_gen\"] = r_A_S - r_A_F\n",
    "    out[\"encoder_gen\"] = r_U_S - r_A_S \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f627681-b926-42de-a436-deecf46e10bc",
   "metadata": {},
   "source": [
    "This is a specific implementation for the evaluation, training, and data. Those functions should can easily be modified for different choices (eg to use sklearn, tune the probe, preprocess the data...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9966cb9c-0b0c-4d2b-acad-be3732c68979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "import os\n",
    "import pdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def eval_model(model, D_test):\n",
    "    \"\"\"Trains a model on D_train and evaluates it on D_test\"\"\"\n",
    "    loader_test = DataLoader(D_test, batch_size=1024, shuffle=False, \n",
    "                             num_workers=os.cpu_count(), pin_memory=True)\n",
    "    \n",
    "    trainer = pl.Trainer(accelerator=\"auto\", logger=False, enable_checkpointing=False)\n",
    "    logs = trainer.test(dataloaders=loader_test, ckpt_path=None, model=model)\n",
    "    pdb.set_trace()\n",
    "\n",
    "    return logs[\"acc\"]\n",
    "\n",
    "def train_eval_model(encoder, D_train, D_test, Probe=torch.nn.Linear, z_dim=z_dim):\n",
    "    \"\"\"Trains a model (encoder and probe) on D_train and evaluates it on D_test\"\"\"\n",
    "    model = Model(encoder, Probe(z_dim, len(train.classes)))\n",
    "    \n",
    "    loader_train = DataLoader(D_train, batch_size=512, shuffle=True, \n",
    "                              num_workers=os.cpu_count(), pin_memory=True)\n",
    "    \n",
    "    trainer = pl.Trainer(logger=False, accelerator=\"auto\", enable_checkpointing=False, \n",
    "                         max_epochs=model.max_epochs)\n",
    "    trainer.fit(model, train_dataloaders=loader_train)\n",
    "    \n",
    "    logs = trainer.test(dataloaders=loader_test, ckpt_path=None, model=model)\n",
    "    \n",
    "    pdb.set_trace()\n",
    "    pass\n",
    "\n",
    "def data_split(D, n, seed=123):\n",
    "    \"\"\"Split a dataset into a set of size n and its complement\"\"\"\n",
    "    \n",
    "    complement_idcs, subset_idcs = train_test_split(\n",
    "        range(len(D)), stratify=D._labels, test_size=n, random_state=seed\n",
    "    )\n",
    "    \n",
    "    return Subset(D, indices=complement_idcs), Subset(D, indices=subset_idcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fb9edf7-b5d5-4496-9509-ef338e3e4534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class Model(pl.LightningModule):\n",
    "    \"\"\"Encoder and Predictor.\"\"\"\n",
    "\n",
    "    def __init__(self, encoder, probe, max_epochs=100):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder.eval()\n",
    "        self.probe = probe #nn.Linear(z_dim, n_classes)\n",
    "        self.max_epochs = max_epochs\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            z = self.encoder(x)\n",
    "        return self.probe(z)\n",
    "\n",
    "    def step(self, batch, name):\n",
    "        x, y = batch\n",
    "        Y_hat = self(x)\n",
    "        acc = accuracy(Y_hat.argmax(dim=-1), y)\n",
    "        self.log(f\"{name}/acc\", acc)\n",
    "        return F.cross_entropy(Y_hat, y.squeeze().long())\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self.step(batch, \"train\")\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self.step(batch, \"test\")\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        optimizer = torch.optim.Adam(self.probe.parameters(), \n",
    "                                     lr=1e-3, weight_decay=1e-6)\n",
    "            \n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, self.max_epochs)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a0e0c5-8729-4499-a543-61a70feadfd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name    | Type           | Params\n",
      "-------------------------------------------\n",
      "0 | encoder | ModifiedResNet | 40.4 M\n",
      "1 | probe   | Linear         | 206 K \n",
      "-------------------------------------------\n",
      "40.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "40.6 M    Total params\n",
      "162.488   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83bbea8a83745daa9b96343e6790707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compute_risk_components(model, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd1f53a-cc8a-44b2-9c53-7fbc8e264ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "riskdec",
   "language": "python",
   "name": "riskdec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
