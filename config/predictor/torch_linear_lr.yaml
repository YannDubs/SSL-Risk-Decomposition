defaults:
  - base_pytorch

name: torch_linear_lr
is_tune_hyperparam: True
arch_kwargs:
  architecture: linear
  is_normalize: False
opt_kwargs:
  weight_decay: 0 # no weight decay for ERM
  scheduler: warmcosine
  optim: sgd

hypopt:
  n_hyper: 20
  metric: loss # uses loss because that gives you max margin classifier => best ERM
  is_tune_on_train: True # you want ERM => tune on train
  search_space:
    to_tune: [lr]
    lr: # loguniform. Note: for adam we divide lr by 100. Note that multiplied by batch_size/256.
      min: 1e-3
      max: 1
  to_eval_first: {}
  kwargs_sampler:
    n_startup_trials: 10
