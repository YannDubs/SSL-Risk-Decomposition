# TODO document each field
# learning_rate : say that for bs 256

### BEIT ###
beitv2_vitB16_pt1k_extractB:
  ssl:
    objective: beit
    ssl_mode: generative
    version: 2
    n_negatives: null
    n_classes: null # it uses vocabulary size of 8192, but that's not global classes
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: null # 2048 for proj and 4096 for pred
    projection_hid_depth: null # 3 for proj and 3 for pred
    architecture_exact: beitb16 cls+avg
    architecture: vitb
    family: vit
    patch_size: 16
    projection_arch: null
    n_parameters: 85761984
    projection_nparameters: null
  representation:
    z_dim: 1536
    z_layer: cls+avg
  optimization:
    epochs: 1600
    batch_size: 2048
    optimizer: adam
    learning_rate:  1.875e-4
    weight_decay: 0.05
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 224
    is_aug_invariant: False
    augmentations:
      - crop
      - color
      - masking
      - normalize
  metrics:
    top1acc_in1k_official: null # 80.1 is when using 300 epochs
  meta:
    where: https://github.com/microsoft/unilm/tree/master/beit2
    notes: the architecture is slightly different than vit because they use relative positional embeddings
    month: 8
    year: 2022
    license: MIT
    is_official: true
  compute:
    n_pus: 16
    pu_type: V100
    time_hours: ???

beitv2_vitL16_pt1k:
  ssl:
    objective: beit
    ssl_mode: generative
    version: 2
    n_negatives: null
    n_classes: null # it uses vocabulary size of 8192, but that's not global classes
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: null # 2048 for proj and 4096 for pred
    projection_hid_depth: null # 3 for proj and 3 for pred
    architecture_exact: beitl16 cls
    architecture: vitl
    family: vit
    patch_size: 16
    projection_arch: null
    n_parameters: 303301632
    projection_nparameters: null
  representation:
    z_dim: 1024
    z_layer: cls
  optimization:
    epochs: 1600
    batch_size: 2048
    optimizer: adam
    learning_rate:  1.875e-4
    weight_decay: 0.05
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 224
    is_aug_invariant: False
    augmentations:
      - crop
      - color
      - masking
      - normalize
  metrics:
    top1acc_in1k_official: null
  meta:
    where: https://github.com/microsoft/unilm/tree/master/beit2
    notes: the architecture is slightly different than vit because they use relative positional embeddings
    month: 8
    year: 2022
    license: MIT
    is_official: true
  compute:
    n_pus: 16
    pu_type: V100
    time_hours: ???

beitv2_vitB16_pt1k:
  ssl:
    objective: beit
    ssl_mode: generative
    version: 2
    n_negatives: null
    n_classes: null # it uses vocabulary size of 8192, but that's not global classes
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: null # 2048 for proj and 4096 for pred
    projection_hid_depth: null # 3 for proj and 3 for pred
    architecture_exact: beitb16 cls
    architecture: vitb
    family: vit
    patch_size: 16
    projection_arch: null
    n_parameters: 85761984
    projection_nparameters: null
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 1600
    batch_size: 2048
    optimizer: adam
    learning_rate:  1.875e-4
    weight_decay: 0.05
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 224
    is_aug_invariant: False
    augmentations:
      - crop
      - color
      - masking
      - normalize
  metrics:
    top1acc_in1k_official: null # 80.1 is when using cls+avg and 300 epochs
  meta:
    where: https://github.com/microsoft/unilm/tree/master/beit2
    notes: the architecture is slightly different than vit because they use relative positional embeddings
    month: 8
    year: 2022
    license: MIT
    is_official: true
  compute:
    n_pus: 16
    pu_type: V100
    time_hours: ???

beitv2_vitB16_pt1k_ep300:
  ssl:
    objective: beit
    ssl_mode: generative
    version: 2
    n_negatives: null
    n_classes: null # it uses vocabulary size of 8192, but that's not global classes
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: null # 2048 for proj and 4096 for pred
    projection_hid_depth: null # 3 for proj and 3 for pred
    architecture_exact: beitb16 cls
    architecture: vitb
    family: vit
    patch_size: 16
    projection_arch: null
    n_parameters: 85761984
    projection_nparameters: null
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 300
    batch_size: 2048
    optimizer: adam
    learning_rate:  1.875e-4
    weight_decay: 0.05
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 224
    is_aug_invariant: False
    augmentations:
      - crop
      - color
      - masking
      - normalize
  metrics:
    top1acc_in1k_official: null # 80.1 is when using cls+avg and 300 epochs
  meta:
    where: https://github.com/microsoft/unilm/tree/master/beit2
    notes: the architecture is slightly different than vit because they use relative positional embeddings
    month: 8
    year: 2022
    license: MIT
    is_official: true
  compute:
    n_pus: 16
    pu_type: V100
    time_hours: ???

beit_vitL16_pt22k:
  ssl:
    objective: beit
    ssl_mode: generative
    version: 1
    n_negatives: null
    n_classes: null # it uses vocabulary size of 8192, but that's not global classes
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: null # 2048 for proj and 4096 for pred
    projection_hid_depth: null # 3 for proj and 3 for pred
    architecture_exact: beitl16 cls
    architecture: vitl
    family: vit
    patch_size: 16
    projection_arch: null
    n_parameters: 303301632
    projection_nparameters: null
  representation:
    z_dim: 1024
    z_layer: cls
  optimization:
    epochs: 800
    batch_size: 2048
    optimizer: adam
    learning_rate:  1.875e-4
    weight_decay: 0.05
    scheduler: cosine
  data:
    pretraining_data: ImageNet-22K
    finetuning_data: null
    img_size: 224
    views: 224
    is_aug_invariant: False
    augmentations:
      - crop
      - masking
      - flipping
      - color
      - normalize
  metrics:
    top1acc_in1k_official: null # 73.5 is when using cls+avg
  meta:
    where: https://github.com/microsoft/unilm/tree/master/beit
    notes: the architecture is slightly different than vit because they use relative positional embeddings
    month: 6
    year: 2021
    license: MIT
    is_official: true
  compute:
    n_pus: 16
    pu_type: V100
    time_hours: 120

beit_vitB16_pt22k:
  ssl:
    objective: beit
    ssl_mode: generative
    version: 1
    n_negatives: null
    n_classes: null # it uses vocabulary size of 8192, but that's not global classes
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: null # 2048 for proj and 4096 for pred
    projection_hid_depth: null # 3 for proj and 3 for pred
    architecture_exact: beitb16 cls
    architecture: vitb
    family: vit
    patch_size: 16
    projection_arch: null
    n_parameters: 85761984
    projection_nparameters: null
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 800
    batch_size: 2048
    optimizer: adam
    learning_rate:  1.875e-4
    weight_decay: 0.05
    scheduler: cosine
  data:
    pretraining_data: ImageNet-22K
    finetuning_data: null
    img_size: 224
    views: 224
    is_aug_invariant: False
    augmentations:
      - crop
      - masking
      - flipping
      - color
      - normalize
  metrics:
    top1acc_in1k_official: null # 56.7 is for cls+avg
  meta:
    where: https://github.com/microsoft/unilm/tree/master/beit
    notes: the architecture is slightly different than vit because they use relative positional embeddings
    month: 6
    year: 2021
    license: MIT
    is_official: true
  compute:
    n_pus: 16
    pu_type: V100
    time_hours: 120


### MMSelfSup ###
deepcluster_rn50_bs512_ep200_mmselfsup:
  ssl:
    objective: deepcluster
    ssl_mode: clustering
    version: 1
    n_negatives: null
    n_classes: 10000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 4e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 512
    optimizer: sgd
    learning_rate: 0.1
    weight_decay: 1e-5
    scheduler: multistep
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 224
    is_aug_invariant: true
    augmentations:
      - crop
      - gray
      - color
      - flip
      - rotate
      - normalize
  metrics:
    top1acc_in1k_official: 46.92
  meta:
    where: https://github.com/open-mmlab/mmselfsup
    notes: null
    month: 7
    year: 2018
    license: Apache 2.0
    is_official: false
  compute:
    n_pus: 8
    pu_type: V100
    time_hours: ???

simclr_rn50_bs256_ep200_mmselfsup:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 512
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 4e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 256
    optimizer: lars #larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 62.56
  meta:
    where: https://github.com/open-mmlab/mmselfsup
    notes: null
    month: 2
    year: 2020
    license: Apache 2.0
    is_official: false
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???

simsiam_rn50_bs256_ep200_mmselfsup:
  ssl:
    objective: simsiam
    ssl_mode: distillation
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: 2048
    projection_hid_depth: 5
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 17e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 256
    optimizer: sgd # larc
    learning_rate: 0.05
    weight_decay: 1e-4
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - gray
      - color
      - flip
      - blur
      - normalize
  metrics:
    top1acc_in1k_official: 69.84
  meta:
    where: https://github.com/open-mmlab/mmselfsup
    notes: null
    month: 10
    year: 2020
    license: Apache 2.0
    is_official: false
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???

densecl_rn50_200ep_mmselfsup:
  ssl:
    objective: odc
    ssl_mode: hierarchical contrastive
    version: 1
    n_negatives: 512
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 256
    projection_hid_width: 512
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 2e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 256
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 1e-4
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: false # it's equivariant
    augmentations:
      - crop
      - gray
      - color
      - flip
      - blur
      - normalize
  metrics:
    top1acc_in1k_official: 63.34
  meta:
    where: https://github.com/open-mmlab/mmselfsup
    notes: null
    month: 9
    year: 2020
    license: Apache 2.0
    is_official: false
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???

odc_rn50_440ep_mmselfsup:
  ssl:
    objective: odc
    ssl_mode: clustering
    version: 1
    n_negatives: null
    n_classes: 10000
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 256
    projection_hid_width: 512
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 2e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 440
    batch_size: 512
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 1e-5
    scheduler: multistep
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - gray
      - color
      - flip
      - rotate
      - normalize
  metrics:
    top1acc_in1k_official: 53.42
  meta:
    where: https://github.com/open-mmlab/mmselfsup
    notes: null
    month: 6
    year: 2020
    license: Apache 2.0
    is_official: false
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???

relativeloc_rn50_70ep_mmselfsup:
  ssl:
    objective: locnet
    ssl_mode: transform # predict the transformation
    version: 1
    n_negatives: null
    n_classes: 8
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 4096
    projection_hid_depth: 3
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 50e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 70
    batch_size: 512
    optimizer: sgd
    learning_rate: 0.1
    weight_decay: 1e-4
    scheduler: multistep
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: 39.65
  meta:
    where: https://github.com/open-mmlab/mmselfsup
    notes: null
    month: 5
    year: 2015
    license: Apache 2.0
    is_official: false
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???



##### Pycontrast #####
infomin_rn50_800ep:
  ssl:
    objective: moco
    ssl_mode: contrastive
    version: 2
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 800
    batch_size: 256
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 1e-4
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - gray
      - blur
      - autoaugment
      - jigsaw
      - normalize
  metrics:
    top1acc_in1k_official: 73.0
  meta:
    where: https://github.com/HobbitLong/PyContrast
    notes: this is simply mocov2 with autoaugment and jigsaw augment
    month: 5
    year: 2020
    license: BSD 3-Clause
    is_official: true
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???

infomin_rn50_200ep:
  ssl:
    objective: moco
    ssl_mode: contrastive
    version: 2
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 256
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 1e-4
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - gray
      - blur
      - autoaugment
      - jigsaw
      - normalize
  metrics:
    top1acc_in1k_official: 70.1
  meta:
    where: https://github.com/HobbitLong/PyContrast
    notes: this is simply mocov2 with autoaugment and jigsaw augment
    month: 5
    year: 2020
    license: BSD 3-Clause
    is_official: true
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???

##### MOCO #####
mocov2_rn50_ep800:
  ssl:
    objective: moco
    ssl_mode: contrastive
    version: 3
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 800
    batch_size: 256
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 1e-4
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - gray
      - blur
      - normalize
  metrics:
    top1acc_in1k_official: 71.1
  meta:
    where: https://github.com/facebookresearch/moco
    notes: null
    month: 3
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 8
    pu_type: V100
    time_hours: 212

mocov2_rn50_ep200:
  ssl:
    objective: moco
    ssl_mode: contrastive
    version: 3
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 256
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 1e-4
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - gray
      - blur
      - normalize
  metrics:
    top1acc_in1k_official: 67.7
  meta:
    where: https://github.com/facebookresearch/moco
    notes: null
    month: 3
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 8
    pu_type: V100
    time_hours: 53

mocov1_rn50_ep200:
  ssl:
    objective: moco
    ssl_mode: contrastive
    version: 3
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 128
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: linear
    n_parameters: 23508032
    projection_nparameters: 0.3e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 256
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 1e-4
    scheduler: multistep
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: 60.6
  meta:
    where: https://github.com/facebookresearch/moco
    notes: null
    month: 11
    year: 2019
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 8
    pu_type: V100
    time_hours: ???

##### MOCOV3 #####
mocov3_vitB_ep300:
  ssl:
    objective: moco
    ssl_mode: contrastive
    version: 3
    n_negatives: 8192
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 5
    architecture_exact: vitb16 cls
    architecture: vitb
    family: vit
    patch_size: 16
    projection_arch: mlp
    n_parameters: 85798656
    projection_nparameters: 28e6
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 300
    batch_size: 4096
    optimizer: adamw
    learning_rate: 1.5e-4
    weight_decay: 0.1
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - blur
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: 76.7
  meta:
    where: https://github.com/facebookresearch/moco-v3
    notes: null
    month: 4
    year: 2021
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 64
    pu_type: Volta
    time_hours: ???

mocov3_vitS_ep300:
  ssl:
    objective: moco
    ssl_mode: contrastive
    version: 3
    n_negatives: 8192
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 5
    architecture_exact: vits16 cls
    architecture: vits
    family: vit
    patch_size: 16
    projection_arch: mlp
    n_parameters: 21665664
    projection_nparameters: 28e6
  representation:
    z_dim: 384
    z_layer: cls
  optimization:
    epochs: 300
    batch_size: 4096
    optimizer: adamw
    learning_rate: 1.5e-4
    weight_decay: 0.1
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - blur
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: 73.2
  meta:
    where: https://github.com/facebookresearch/moco-v3
    notes: moco uses 12 heads instead of the standard 6
    month: 4
    year: 2021
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 64
    pu_type: Volta
    time_hours: ???

mocov3_rn50_ep1000:
  ssl:
    objective: moco
    ssl_mode: contrastive
    version: 3
    n_negatives: 8192
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 5
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 28e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 4096
    optimizer: lars
    learning_rate:  0.6
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - blur
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: 74.6
  meta:
    where: https://github.com/facebookresearch/moco-v3
    notes: null
    month: 4
    year: 2021
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 16
    pu_type: Volta
    time_hours: ???

mocov3_rn50_ep300:
  ssl:
    objective: moco
    ssl_mode: contrastive
    version: 3
    n_negatives: 8192
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 5
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 28e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 300
    batch_size: 4096
    optimizer: lars
    learning_rate:  0.6
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - blur
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: 72.8
  meta:
    where: https://github.com/facebookresearch/moco-v3
    notes: null
    month: 4
    year: 2021
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 16
    pu_type: Volta
    time_hours: ???

mocov3_rn50_ep100:
  ssl:
    objective: moco
    ssl_mode: contrastive
    version: 3
    n_negatives: 8192
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 5
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 28e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 4096
    optimizer: lars
    learning_rate:  0.6
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - blur
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: 68.9
  meta:
    where: https://github.com/facebookresearch/moco-v3
    notes: null
    month: 4
    year: 2021
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 16
    pu_type: Volta
    time_hours: ???


##### MSN #####
msn_vitl7_ep200:
  ssl:
    objective: msn
    ssl_mode: clustering
    version: 1
    n_negatives: 2048
    n_classes: 1024
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitl7 cls
    architecture: vitl
    family: vit
    patch_size: 7
    projection_arch: mlp
    n_parameters: 303301632
    projection_nparameters: 13e6
  representation:
    z_dim: 1024
    z_layer: cls
  optimization:
    epochs: 200
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.001
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10*96
    is_aug_invariant: true
    augmentations:
      - crop
      - masking
      - flip
      - color
      - blur
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/facebookresearch/msn
    notes: null
    month: 4
    year: 2022
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: ???
    pu_type: ???
    time_hours: ???

msn_vitl16_ep600:
  ssl:
    objective: msn
    ssl_mode: clustering
    version: 1
    n_negatives: 2048
    n_classes: 1024
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitl16 cls
    architecture: vitl
    family: vit
    patch_size: 16
    projection_arch: mlp
    n_parameters: 303301632
    projection_nparameters: 13e6
  representation:
    z_dim: 1024
    z_layer: cls
  optimization:
    epochs: 300
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.001
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10*96
    is_aug_invariant: true
    augmentations:
      - crop
      - masking
      - flip
      - color
      - blur
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: 80.7
  meta:
    where: https://github.com/facebookresearch/msn
    notes: null
    month: 4
    year: 2022
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: ???
    pu_type: ???
    time_hours: ???

msn_vitb4_ep300:
  ssl:
    objective: msn
    ssl_mode: clustering
    version: 1
    n_negatives: 2048
    n_classes: 1024
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitb4 cls
    architecture: vitb
    family: vit
    patch_size: 4
    projection_arch: mlp
    n_parameters: 85798656
    projection_nparameters: 13e6
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 300
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.001
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10*96
    is_aug_invariant: true
    augmentations:
      - crop
      - masking
      - flip
      - color
      - blur
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/facebookresearch/msn
    notes: null
    month: 4
    year: 2022
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: ???
    pu_type: ???
    time_hours: ???


msn_vitb16_ep600:
  ssl:
    objective: msn
    ssl_mode: clustering
    version: 1
    n_negatives: 2048
    n_classes: 1024
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitb16 cls
    architecture: vitb
    family: vit
    patch_size: 16
    projection_arch: mlp
    n_parameters: 85798656
    projection_nparameters: 13e6
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 600
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.001
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10*96
    is_aug_invariant: true
    augmentations:
      - crop
      - masking
      - flip
      - color
      - blur
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/facebookresearch/msn
    notes: null
    month: 4
    year: 2022
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 16
    pu_type: ???
    time_hours: ???


msn_vits16_ep800:
  ssl:
    objective: msn
    ssl_mode: clustering
    version: 1
    n_negatives: 2048
    n_classes: 1024
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vits16 cls
    architecture: vits
    family: vit
    patch_size: 16
    projection_arch: mlp
    n_parameters: 21665664
    projection_nparameters: 13e6
  representation:
    z_dim: 384
    z_layer: cls
  optimization:
    epochs: 800
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.001
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10*96
    is_aug_invariant: true
    augmentations:
      - crop
      - masking
      - flip
      - color
      - blur
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: 76.9
  meta:
    where: https://github.com/facebookresearch/msn
    notes: null
    month: 4
    year: 2022
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 16
    pu_type: ???
    time_hours: ???


##### MAE #####
mae_vitH14:
  ssl:
    objective: mae
    ssl_mode: generative
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: null # 2048 for proj and 4096 for pred
    projection_hid_depth: null # 3 for proj and 3 for pred
    architecture_exact: vith14 cls
    architecture: vith
    family: vit
    patch_size: 14
    projection_arch: null
    n_parameters: 630764800
    projection_nparameters: null
  representation:
    z_dim: 1280
    z_layer: cls
  optimization:
    epochs: 1600
    batch_size: 4096
    optimizer: adamw
    learning_rate:  1.5e-4
    weight_decay: 0.05
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 224
    is_aug_invariant: False
    augmentations:
      - crop
      - masking
      - normalize
  metrics:
    top1acc_in1k_official: 76.6
  meta:
    where: https://github.com/facebookresearch/mae
    notes: null
    month: 11
    year: 2021
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 128
    pu_type: TPU v3
    time_hours: 78

mae_vitL16:
  ssl:
    objective: mae
    ssl_mode: generative
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: null # 2048 for proj and 4096 for pred
    projection_hid_depth: null # 3 for proj and 3 for pred
    architecture_exact: vitl16 cls
    architecture: vitl
    family: vit
    patch_size: 16
    projection_arch: null
    n_parameters: 303301632
    projection_nparameters: null
  representation:
    z_dim: 1024
    z_layer: cls
  optimization:
    epochs: 1600
    batch_size: 4096
    optimizer: adamw
    learning_rate:  1.5e-4
    weight_decay: 0.05
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 224
    is_aug_invariant: False
    augmentations:
      - crop
      - masking
      - normalize
  metrics:
    top1acc_in1k_official: 75.8
  meta:
    where: https://github.com/facebookresearch/mae
    notes: null
    month: 11
    year: 2021
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 128
    pu_type: TPU v3
    time_hours: 31

mae_vitB16:
  ssl:
    objective: mae
    ssl_mode: generative
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: null # 2048 for proj and 4096 for pred
    projection_hid_depth: null # 3 for proj and 3 for pred
    architecture_exact: vitb16 cls
    architecture: vitb
    family: vit
    patch_size: 16
    projection_arch: null
    n_parameters: 85798656
    projection_nparameters: null
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 1600
    batch_size: 4096
    optimizer: adamw
    learning_rate:  1.5e-4
    weight_decay: 0.05
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 224
    is_aug_invariant: False
    augmentations:
      - crop
      - masking
      - normalize
  metrics:
    top1acc_in1k_official: 68.0
  meta:
    where: https://github.com/facebookresearch/mae
    notes: null
    month: 11
    year: 2021
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 128
    pu_type: TPU v3
    time_hours: ???

##### MUGS #####
mugs_vitb16_ep400_extractB:
  ssl:
    objective: mugs
    ssl_mode: hierarchical contrastive # both global and low level features (contrast patches)
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048 # 2048 for proj and 4096 for pred
    projection_hid_depth: 6 # 3 for proj and 3 for pred
    architecture_exact: vits16 cls+avg
    architecture: vits
    family: vit
    patch_size: 16
    projection_arch: mlp
    n_parameters: 85798656
    projection_nparameters: 73e6
  representation:
    z_dim: 1536
    z_layer: cls+avg
  optimization:
    epochs: 400
    batch_size: 1008
    optimizer: adamw
    learning_rate:  0.0008
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarization
      - autoaugment # for student
      - erasing
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 78.0
  meta:
    where: https://github.com/sail-sg/mugs
    notes: evaluation of the official model is unclear and eval code is not available but I think it's like DINO.
    month: 3
    year: 2022
    license: Apache License 2.0
    is_official: true
  compute:
    n_pus: 24
    pu_type: A100
    time_hours: 64

mugs_vits16_ep800_extractS:
  ssl:
    objective: mugs
    ssl_mode: hierarchical contrastive # both global and low level features (contrast patches)
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048 # 2048 for proj and 4096 for pred
    projection_hid_depth: 6 # 3 for proj and 3 for pred
    architecture_exact: vits16 4xcls
    architecture: vits
    family: vit
    patch_size: 16
    projection_arch: mlp
    n_parameters: 21665664
    projection_nparameters: 73e6
  representation:
    z_dim: 1536
    z_layer: 4xcls
  optimization:
    epochs: 800
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.0008
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarization
      - autoaugment # for student
      - erasing
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 75.6
  meta:
    where: https://github.com/sail-sg/mugs
    notes: evaluation of the official model is unclear and eval code is not available but I think it's like DINO.
    month: 3
    year: 2022
    license: Apache License 2.0
    is_official: true
  compute:
    n_pus: 16
    pu_type: A100
    time_hours: 81

mugs_vitl16_ep250:
  ssl:
    objective: mugs
    ssl_mode: hierarchical contrastive # both global and low level features (contrast patches)
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048 # 2048 for proj and 4096 for pred
    projection_hid_depth: 6 # 3 for proj and 3 for pred
    architecture_exact: vitl16 cls
    architecture: vitl
    family: vit
    patch_size: 16
    projection_arch: mlp
    n_parameters: 303301632
    projection_nparameters: 73e6
  representation:
    z_dim: 1024
    z_layer: cls
  optimization:
    epochs: 400
    batch_size: 640
    optimizer: adamw
    learning_rate:  0.0015
    weight_decay: 0.025
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarization
      - autoaugment # for student
      - erasing
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/sail-sg/mugs
    notes: evaluation of the official model is unclear and eval code is not available but I think it's like DINO.
    month: 3
    year: 2022
    license: Apache License 2.0
    is_official: true
  compute:
    n_pus: 40
    pu_type: A100
    time_hours: 120

mugs_vitb16_ep400:
  ssl:
    objective: mugs
    ssl_mode: hierarchical contrastive # both global and low level features (contrast patches)
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048 # 2048 for proj and 4096 for pred
    projection_hid_depth: 6 # 3 for proj and 3 for pred
    architecture_exact: vitb16 cls
    architecture: vitb
    family: vit
    patch_size: 16
    projection_arch: mlp
    n_parameters: 85798656
    projection_nparameters: 73e6
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 400
    batch_size: 1008
    optimizer: adamw
    learning_rate:  0.0008
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarization
      - autoaugment # for student
      - erasing
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/sail-sg/mugs
    notes: evaluation of the official model is unclear and eval code is not available but I think it's like DINO.
    month: 3
    year: 2022
    license: Apache License 2.0
    is_official: true
  compute:
    n_pus: 24
    pu_type: A100
    time_hours: 64

mugs_vits16_ep800:
  ssl:
    objective: mugs
    ssl_mode: hierarchical contrastive # both global and low level features (contrast patches)
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048 # 2048 for proj and 4096 for pred
    projection_hid_depth: 6 # 3 for proj and 3 for pred
    architecture_exact: vits16 cls
    architecture: vits
    family: vit
    patch_size: 16
    projection_arch: mlp
    n_parameters: 21665664
    projection_nparameters: 73e6
  representation:
    z_dim: 384
    z_layer: cls
  optimization:
    epochs: 800
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.0008
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarization
      - autoaugment # for student
      - erasing
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/sail-sg/mugs
    notes: evaluation of the official model is unclear and eval code is not available but I think it's like DINO.
    month: 3
    year: 2022
    license: Apache License 2.0
    is_official: true
  compute:
    n_pus: 16
    pu_type: A100
    time_hours: 81

mugs_vits16_ep300:
  ssl:
    objective: mugs
    ssl_mode: hierarchical contrastive # both global and low level features (contrast patches)
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048 # 2048 for proj and 4096 for pred
    projection_hid_depth: 6 # 3 for proj and 3 for pred
    architecture_exact: vits16 cls
    architecture: vits
    family: vit
    patch_size: 16
    projection_arch: mlp
    n_parameters: 21665664
    projection_nparameters: 73e6
  representation:
    z_dim: 384
    z_layer: cls
  optimization:
    epochs: 300
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.0008
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarization
      - autoaugment # for student
      - erasing
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/sail-sg/mugs
    notes: evaluation of the official model is unclear and eval code is not available but I think it's like DINO.
    month: 3
    year: 2022
    license: Apache License 2.0
    is_official: true
  compute:
    n_pus: 16
    pu_type: A100
    time_hours: 108

mugs_vits16_ep100:
  ssl:
    objective: mugs
    ssl_mode: hierarchical contrastive # both global and low level features (contrast patches)
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048 # 2048 for proj and 4096 for pred
    projection_hid_depth: 6 # 3 for proj and 3 for pred
    architecture_exact: vits16 cls
    architecture: vits
    family: vit
    patch_size: 16
    projection_arch: mlp
    n_parameters: 21665664
    projection_nparameters: 73e6
  representation:
    z_dim: 384
    z_layer: cls
  optimization:
    epochs: 100
    batch_size: 512
    optimizer: adamw
    learning_rate:  0.0008
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarization
      - autoaugment # for student
      - erasing
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/sail-sg/mugs
    notes: evaluation of the official model is unclear and eval code is not available but I think it's like DINO.
    month: 3
    year: 2022
    license: Apache License 2.0
    is_official: true
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: 27

##### IBOT #####
ibot_vitS16_extractS:
  ssl:
    objective: ibot
    ssl_mode: clustering #actually it's clustering of patches => not really clustering
    version: 1
    n_negatives: 2048
    n_classes: 8192
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vits16 4xcls
    architecture: vits
    family: vit
    patch_size: 16
    projection_arch: mlp
    n_parameters: 85798656
    projection_nparameters: 13e6
  representation:
    z_dim: 1536
    z_layer: 4xcls
  optimization:
    epochs: 800
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.0005
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarization
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 77.9
  meta:
    where: https://github.com/bytedance/ibot
    notes: prediction shape is block
    month: 11
    year: 2021
    license: Apache License 2.0
    is_official: true
  compute:
    n_pus: 16
    pu_type: V100
    time_hours: 193.4

ibot_vitB16_extractB:
  ssl:
    objective: ibot
    ssl_mode: clustering #actually it's clustering of patches => not really clustering
    version: 1
    n_negatives: 2064
    n_classes: 8192
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitb16 cls+avg
    architecture: vitb
    family: vit
    patch_size: 16
    projection_arch: mlp
    n_parameters: 85798656
    projection_nparameters: 13e6
  representation:
    z_dim: 1536
    z_layer: cls+avg
  optimization:
    epochs: 400
    batch_size: 1032
    optimizer: adamw
    learning_rate:  0.00075
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: prediction shape is block
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarization
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 79.5
  meta:
    where: https://github.com/bytedance/ibot
    notes: null
    month: 11
    year: 2021
    license: Apache License 2.0
    is_official: true
  compute:
    n_pus: 24
    pu_type: V100
    time_hours: ???

ibot_vitL16:
  ssl:
    objective: ibot
    ssl_mode: clustering #actually it's clustering of patches => not really clustering
    version: 1
    n_negatives: 2048
    n_classes: 8192
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitl16 cls
    architecture: vitl
    family: vit
    patch_size: 16
    projection_arch: mlp
    n_parameters: 303301632
    projection_nparameters: 13e6
  representation:
    z_dim: 1024
    z_layer: cls
  optimization:
    epochs: 250
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.0005
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarization
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/bytedance/ibot
    notes: prediction shape is block
    month: 11
    year: 2021
    license: Apache License 2.0
    is_official: true
  compute:
    n_pus: 64
    pu_type: V100
    time_hours: ???

ibot_vitS16:
  ssl:
    objective: ibot
    ssl_mode: clustering #actually it's clustering of patches => not really clustering
    version: 1
    n_negatives: 2048
    n_classes: 8192
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vits16 cls
    architecture: vits
    family: vit
    patch_size: 16
    projection_arch: mlp
    n_parameters: 21665664
    projection_nparameters: 13e6
  representation:
    z_dim: 384
    z_layer: cls
  optimization:
    epochs: 800
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.0005
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarization
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/bytedance/ibot
    notes: prediction shape is block
    month: 11
    year: 2021
    license: Apache License 2.0
    is_official: true
  compute:
    n_pus: 16
    pu_type: V100
    time_hours: 193.4

ibot_vitB16:
  ssl:
    objective: ibot
    ssl_mode: clustering #actually it's clustering of patches => not really clustering
    version: 1
    n_negatives: 2064
    n_classes: 8192
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitb16 cls
    architecture: vitb
    family: vit
    patch_size: 16
    projection_arch: mlp
    n_parameters: 85798656
    projection_nparameters: 13e6
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 400
    batch_size: 1032
    optimizer: adamw
    learning_rate:  0.00075
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: prediction shape is block
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarization
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/bytedance/ibot
    notes: null
    month: 11
    year: 2021
    license: Apache License 2.0
    is_official: true
  compute:
    n_pus: 24
    pu_type: V100
    time_hours: ???

##### DINO #####
dino_vitB16_extractS:
  ssl:
    objective: dino
    ssl_mode: clustering
    version: 1
    n_negatives: 2048
    n_classes: 65536
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitb16 4xcls
    architecture: vitb
    family: vit
    patch_size: 16
    projection_arch: mlp
    n_parameters: 85798656
    projection_nparameters: 26e6
  representation:
    z_dim: 3072
    z_layer: 4xcls
  optimization:
    epochs: 400
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.00075
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - solarization
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 78.2
  meta:
    where: https://github.com/facebookresearch/dino
    notes: null
    month: 4
    year: 2021
    license: Apache License 2.0
    is_official: true
  compute:
    n_pus: 32
    pu_type: V100
    time_hours: ???

dino_vitS16_extractB:
  ssl:
    objective: dino
    ssl_mode: clustering
    version: 1
    n_negatives: 2048
    n_classes: 65536
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vits16 cls+avg
    architecture: vits
    family: vit
    patch_size: 16
    projection_arch: mlp
    n_parameters: 21665664
    projection_nparameters: 26e6
  representation:
    z_dim: 768
    z_layer: cls+avg
  optimization:
    epochs: 800
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.0005
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - solarization
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/facebookresearch/dino
    notes: null
    month: 4
    year: 2021
    license: Apache License 2.0
    is_official: true
  compute:
    n_pus: 16
    pu_type: V100
    time_hours: 194

dino_vitB8:
  ssl:
    objective: dino
    ssl_mode: clustering
    version: 1
    n_negatives: 2112
    n_classes: 65536
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitb8 cls+avg
    architecture: vitb
    family: vit
    patch_size: 8
    projection_arch: mlp
    n_parameters: 85807872
    projection_nparameters: 26e6
  representation:
    z_dim: 1536
    z_layer: cls+avg
  optimization:
    epochs: 300
    batch_size: 1056
    optimizer: adamw
    learning_rate:  0.0005
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - solarization
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 80.1
  meta:
    where: https://github.com/facebookresearch/dino
    notes: null
    month: 4
    year: 2021
    license: Apache License 2.0
    is_official: true
  compute:
    n_pus: 176
    pu_type: V100
    time_hours: ???

dino_vitB16:
  ssl:
    objective: dino
    ssl_mode: clustering
    version: 1
    n_negatives: 2048
    n_classes: 65536
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitb16 cls+avg
    architecture: vitb
    family: vit
    patch_size: 16
    projection_arch: mlp
    n_parameters: 85798656
    projection_nparameters: 26e6
  representation:
    z_dim: 1536
    z_layer: cls+avg
  optimization:
    epochs: 400
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.00075
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - solarization
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 78.2
  meta:
    where: https://github.com/facebookresearch/dino
    notes: null
    month: 4
    year: 2021
    license: Apache License 2.0
    is_official: true
  compute:
    n_pus: 32
    pu_type: V100
    time_hours: ???

dino_vitS16:
  ssl:
    objective: dino
    ssl_mode: clustering
    version: 1
    n_negatives: 2048
    n_classes: 65536
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vits16 4xcls
    architecture: vits
    family: vit
    patch_size: 16
    projection_arch: mlp
    n_parameters: 21665664
    projection_nparameters: 26e6
  representation:
    z_dim: 1536
    z_layer: 4xcls
  optimization:
    epochs: 800
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.0005
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - solarization
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 77.0
  meta:
    where: https://github.com/facebookresearch/dino
    notes: null
    month: 4
    year: 2021
    license: Apache License 2.0
    is_official: true
  compute:
    n_pus: 16
    pu_type: V100
    time_hours: 194

dino_vitB8_last:
  ssl:
    objective: dino
    ssl_mode: clustering
    version: 1
    n_negatives: 2112
    n_classes: 65536
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitb8 cls
    architecture: vitb
    family: vit
    patch_size: 8
    projection_arch: mlp
    n_parameters: 85807872
    projection_nparameters: 26e6
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 300
    batch_size: 1056
    optimizer: adamw
    learning_rate:  0.0005
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - solarization
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/facebookresearch/dino
    notes: null
    month: 4
    year: 2021
    license: Apache License 2.0
    is_official: true
  compute:
    n_pus: 176
    pu_type: V100
    time_hours: ???

dino_vitB16_last:
  ssl:
    objective: dino
    ssl_mode: clustering
    version: 1
    n_negatives: 2048
    n_classes: 65536
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitb16 cls
    architecture: vitb
    family: vit
    patch_size: 16
    projection_arch: mlp
    n_parameters: 85798656
    projection_nparameters: 26e6
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 400
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.00075
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - solarization
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/facebookresearch/dino
    notes: null
    month: 4
    year: 2021
    license: Apache License 2.0
    is_official: true
  compute:
    n_pus: 32
    pu_type: V100
    time_hours: ???

dino_vitS8_last:
  ssl:
    objective: dino
    ssl_mode: clustering
    version: 1
    n_negatives: 2048
    n_classes: 65536
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vits8 cls
    architecture: vits
    family: vit
    patch_size: 8
    projection_arch: mlp
    n_parameters: 21670272
    projection_nparameters: 26e6
  representation:
    z_dim: 384
    z_layer: cls
  optimization:
    epochs: 800
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.0005
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - solarization
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/facebookresearch/dino
    notes: null
    month: 4
    year: 2021
    license: Apache License 2.0
    is_official: true
  compute:
    n_pus: 64
    pu_type: V100
    time_hours: ???

dino_vitS16_last:
  ssl:
    objective: dino
    ssl_mode: clustering
    version: 1
    n_negatives: 2048
    n_classes: 65536
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vits16 cls
    architecture: vits
    family: vit
    patch_size: 16
    projection_arch: mlp
    n_parameters: 21665664
    projection_nparameters: 26e6
  representation:
    z_dim: 384
    z_layer: cls
  optimization:
    epochs: 800
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.0005
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - solarization
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/facebookresearch/dino
    notes: null
    month: 4
    year: 2021
    license: Apache License 2.0
    is_official: true
  compute:
    n_pus: 16
    pu_type: V100
    time_hours: 194

dino_rn50:
  ssl:
    objective: dino
    ssl_mode: clustering
    version: 1
    n_negatives: 8160
    n_classes: 60000
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 24e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 800
    batch_size: 4080
    optimizer: lars
    learning_rate:  0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+6x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - solarization
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 75.3
  meta:
    where: https://github.com/facebookresearch/dino
    notes: null
    month: 4
    year: 2021
    license: Apache License 2.0
    is_official: true
  compute:
    n_pus: 80
    pu_type: V100
    time_hours: ???


##### CLIP #####
clip_vitL14_px336:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 768
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vitl14 cls
    architecture: vitl
    family: vit
    patch_size: 14
    projection_arch: linear
    n_parameters: 303507456
    projection_nparameters: 1e6
  representation:
    z_dim: 1024
    z_layer: cls
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 4e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: null
    img_size: 336
    views: 2x336
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 85.4
  meta:
    where: https://github.com/openai/CLIP
    notes: null
    month: 1
    year: 2021
    license: MIT
    is_official: true
  compute:
    n_pus: 256
    pu_type: V100
    time_hours: 288

clip_vitL14:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 768
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vitl14 cls
    architecture: vitl
    family: vit
    patch_size: 14
    projection_arch: linear
    n_parameters: 303179776
    projection_nparameters: 1e6
  representation:
    z_dim: 1024
    z_layer: cls
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 4e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 83.9
  meta:
    where: https://github.com/openai/CLIP
    notes: null
    month: 1
    year: 2021
    license: MIT
    is_official: true
  compute:
    n_pus: ???
    pu_type: V100
    time_hours: ???

clip_vitB32:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vitb32 cls
    architecture: vitb
    family: vit
    patch_size: 32
    projection_arch: linear
    n_parameters: 85799424
    projection_nparameters: 0.4e6
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 5e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 76.1
  meta:
    where: https://github.com/openai/CLIP
    notes: null
    month: 1
    year: 2021
    license: MIT
    is_official: true
  compute:
    n_pus: ???
    pu_type: V100
    time_hours: ???

clip_vitB16:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vitb16 cls
    architecture: vitb
    family: vit
    patch_size: 16
    projection_arch: linear
    n_parameters: 85799424
    projection_nparameters: 0.4e6
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 5e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 80.2
  meta:
    where: https://github.com/openai/CLIP
    notes: null
    month: 1
    year: 2021
    license: MIT
    is_official: true
  compute:
    n_pus: ???
    pu_type: V100
    time_hours: ???

clip_rn101:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet101
    architecture: resnet101
    family: resnet
    patch_size: null
    projection_arch: linear
    n_parameters: 59407200
    projection_nparameters: 2e6
  representation:
    z_dim: 2048
    z_layer: attnpool
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 3.6e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 75.7
  meta:
    where: https://github.com/openai/CLIP
    notes: unclear for resnet if CLIP uses features before or after the projection head. We use before
    month: 1
    year: 2021
    license: MIT
    is_official: true
  compute:
    n_pus: ???
    pu_type: V100
    time_hours: ???

clip_rn50x64:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 1024
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50w64
    architecture: resnet50w64
    family: resnet
    patch_size: null
    projection_arch: linear
    n_parameters: 432966336
    projection_nparameters: 2e6
  representation:
    z_dim: 4096
    z_layer: attnpool
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 3.6e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: null
    img_size: 448
    views: 2x448
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 83.6
  meta:
    where: https://github.com/openai/CLIP
    notes: unclear for resnet if CLIP uses features before or after the projection head. We use before
    month: 1
    year: 2021
    license: MIT
    is_official: true
  compute:
    n_pus: 592
    pu_type: V100
    time_hours: 432

clip_rn50x16:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 768
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50w16
    architecture: resnet50w16
    family: resnet
    patch_size: null
    projection_arch: linear
    n_parameters: 174409104
    projection_nparameters: 2e6
  representation:
    z_dim: 3072
    z_layer: attnpool
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 4e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: null
    img_size: 384
    views: 2x384
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 81.5
  meta:
    where: https://github.com/openai/CLIP
    notes: unclear for resnet if CLIP uses features before or after the projection head. We use before
    month: 1
    year: 2021
    license: MIT
    is_official: true
  compute:
    n_pus: ???
    pu_type: V100
    time_hours: ???

clip_rn50x4:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 1024
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50w4
    architecture: resnet50w4
    family: resnet
    patch_size: null
    projection_arch: linear
    n_parameters: 92054200
    projection_nparameters: 2e6
  representation:
    z_dim: 640
    z_layer: attnpool
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 5e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: null
    img_size: 288
    views: 2x288
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 78.2
  meta:
    where: https://github.com/openai/CLIP
    notes: unclear for resnet if CLIP uses features before or after the projection head. We use before
    month: 1
    year: 2021
    license: MIT
    is_official: true
  compute:
    n_pus: ???
    pu_type: V100
    time_hours: ???

clip_rn50:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 1024
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: linear
    n_parameters: 38317920
    projection_nparameters: 2e6
  representation:
    z_dim: 2048
    z_layer: attnpool
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 5e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 73.3
  meta:
    where: https://github.com/openai/CLIP
    notes: unclear for resnet if CLIP uses features before or after the projection head. We use before
    month: 1
    year: 2021
    license: MIT
    is_official: true
  compute:
    n_pus: ???
    pu_type: V100
    time_hours: ???

##### Lossyless #####
lossyless_b01:
  ssl:
    objective: clip+eb
    ssl_mode: contrastive
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vitb32 cls
    architecture: vitb
    family: vit
    patch_size: 32
    projection_arch: linear
    n_parameters: 85799424
    projection_nparameters: 0.4e6
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 5e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: mscoco
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/YannDubs/lossyless
    notes: the only difference compared to clip is an invertible linear layer one top of representations that was trained on mscoco to compress the representations. b denotes `beta`, larger means more compression.
    month: 6
    year: 2021
    license: MIT
    is_official: true
  compute:
    n_pus: ???
    pu_type: V100
    time_hours: ???

lossyless_b005:
  ssl:
    objective: clip+eb
    ssl_mode: contrastive
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vitb32 cls
    architecture: vitb
    family: vit
    patch_size: 32
    projection_arch: linear
    n_parameters: 85799424
    projection_nparameters: 0.4e6
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 5e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: mscoco
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/YannDubs/lossyless
    notes: the only difference compared to clip is an invertible linear layer one top of representations that was trained on mscoco to compress the representations. b denotes `beta`, larger means more compression.
    month: 6
    year: 2021
    license: MIT
    is_official: true
  compute:
    n_pus: ???
    pu_type: V100
    time_hours: ???

lossyless_b001:
  ssl:
    objective: clip+eb
    ssl_mode: contrastive
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vitb32 cls
    architecture: vitb
    family: vit
    patch_size: 32
    projection_arch: linear
    n_parameters: 85799424
    projection_nparameters: 0.4e6
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 5e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: mscoco
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/YannDubs/lossyless
    notes: the only difference compared to clip is an invertible linear layer one top of representations that was trained on mscoco to compress the representations. b denotes `beta`, larger means more compression.
    month: 6
    year: 2021
    license: MIT
    is_official: true
  compute:
    n_pus: ???
    pu_type: V100
    time_hours: ???

##### Risk Dec #####
speccl_bs384_ep100:
  ssl:
    objective: speccl
    ssl_mode: contrastive
    version: 1
    n_negatives: 5120
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 8192
    projection_hid_width: 8192
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 31128640
    projection_nparameters: 151e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 384
    optimizer: sgd # larc
    learning_rate: 0.033
    weight_decay: 1e-4
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 66.97
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes: null
    month: 6
    year: 2022
    license: MIT
    is_official: true
  compute:
    n_pus: 8
    pu_type: GTX 1080
    time_hours: ???

simclr_resnet50_d8192_e100_m2:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 5120
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50d8192
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 31128640
    projection_nparameters: 17e6
  representation:
    z_dim: 8192
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes: null
    month: 2
    year: 2020
    license: MIT
    is_official: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

simclr_resnet50_dNone_e100_m2_headTNoneSNone:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 5120
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 2048
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: null
    n_parameters: 23508032
    projection_nparameters: 0
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes: no projection head
    month: 2
    year: 2020
    license: MIT
    is_official: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

simclr_resnet50_dNone_e100_m2_headTMlpSMlp:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 5120
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: linear
    n_parameters: 23508032
    projection_nparameters: 9e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes: both teacher and student have same mlp projection heads but weights are not shared
    month: 2
    year: 2020
    license: MIT
    is_official: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

simclr_resnet50_dNone_e100_m2_headTMlpSLin:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 5120
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: linear
    n_parameters: 23508032
    projection_nparameters: 9e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes: student has linear head
    month: 2
    year: 2020
    license: MIT
    is_official: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

simclr_resnet50_dNone_e100_m2_headTLinSLin:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 5120
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: linear
    n_parameters: 23508032
    projection_nparameters: 0.3e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes: both teacher and student have linear projection heads
    month: 2
    year: 2020
    license: MIT
    is_official: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

simclr_resnet50_dNone_e100_m2_data030:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 5120
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 9e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 300
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: 30% ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes:  trains on 30% of imagenet but multiplies by 3 number of epochs to keep same steps.
    month: 2
    year: 2020
    license: MIT
    is_official: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

simclr_resnet50_dNone_e100_m2_data010:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 5120
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 9e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: 10% ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes:  trains on 10% of imagenet but multiplies by 10 number of epochs to keep same steps.
    month: 2
    year: 2020
    license: MIT
    is_official: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

simclr_resnet50_dNone_e100_m2:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 5120
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 9e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes:  null
    month: 6
    year: 2022
    license: MIT
    is_official: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

dissl_resnet50_d4096_e100_m2:
  ssl:
    objective: dissl
    ssl_mode: clustering
    version: 1
    n_negatives: 5120
    n_classes: 16384
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50d4096
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 29023296
    projection_nparameters: 16e6
  representation:
    z_dim: 4096
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes:  null
    month: 6
    year: 2022
    license: MIT
    is_official: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

dissl_resnet50_dNone_e100_m2_headTMlpSMlp:
  ssl:
    objective: dissl
    ssl_mode: clustering
    version: 1
    n_negatives: 5120
    n_classes: 16384
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 17e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes:  uses mlp projection head for teacher and student
    month: 6
    year: 2022
    license: MIT
    is_official: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

dissl_resnet50_dNone_e100_m2_headTLinSLin:
  ssl:
    objective: dissl
    ssl_mode: clustering
    version: 1
    n_negatives: 5120
    n_classes: 16384
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: linear
    n_parameters: 23508032
    projection_nparameters: 9e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes:  uses linear projection head for teacher and student
    month: 6
    year: 2022
    license: MIT
    is_official: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

dissl_resnet50_dNone_e100_m2_augSmall:
  ssl:
    objective: dissl
    ssl_mode: clustering
    version: 1
    n_negatives: 5120
    n_classes: 16384
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 17e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes: decreases strength of crop and color distortion
    month: 6
    year: 2022
    license: MIT
    is_official: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

dissl_resnet50_dNone_e100_m2_augLarge:
  ssl:
    objective: dissl
    ssl_mode: clustering
    version: 1
    n_negatives: 5120
    n_classes: 16384
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 17e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes: increases strength of crop and color distortion
    month: 6
    year: 2022
    license: MIT
    is_official: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

##### ISSL #####
dissl_resnet50_d8192_e800_m8:
  ssl:
    objective: dissl
    ssl_mode: clustering
    version: 1
    n_negatives: 5120
    n_classes: 16384
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50d8192
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 31128640
    projection_nparameters: 17e6
  representation:
    z_dim: 8192
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+6x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 73.9
  meta:
    where: https://github.com/YannDubs/Invariant-Self-Supervised-Learning
    notes: null
    month: 6
    year: 2022
    license: MIT
    is_official: true
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

dissl_resnet50_d8192_e400_m6:
  ssl:
    objective: dissl
    ssl_mode: clustering
    version: 1
    n_negatives: 5120
    n_classes: 16384
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50d8192
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 31128640
    projection_nparameters: 17e6
  representation:
    z_dim: 8192
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x160+4x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 74.0
  meta:
    where: https://github.com/YannDubs/Invariant-Self-Supervised-Learning
    notes: null
    month: 6
    year: 2022
    license: MIT
    is_official: true
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

dissl_resnet50_dNone_e400_m6:
  ssl:
    objective: dissl
    ssl_mode: clustering
    version: 1
    n_negatives: 5120
    n_classes: 16384
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 17e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x160+4x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 73.0
  meta:
    where: https://github.com/YannDubs/Invariant-Self-Supervised-Learning
    notes: null
    month: 6
    year: 2022
    license: MIT
    is_official: true
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

dissl_resnet50_dNone_e400_m2:
  ssl:
    objective: dissl
    ssl_mode: clustering
    version: 1
    n_negatives: 5120
    n_classes: 16384
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 31128640
    projection_nparameters: 17e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 71.1
  meta:
    where: https://github.com/YannDubs/Invariant-Self-Supervised-Learning
    notes: null
    month: 6
    year: 2022
    license: MIT
    is_official: true
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

dissl_resnet50_d8192_e100_m2:
  ssl:
    objective: dissl
    ssl_mode: clustering
    version: 1
    n_negatives: 5120
    n_classes: 16384
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50d8192
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 31128640
    projection_nparameters: 17e6
  representation:
    z_dim: 8192
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 68.9
  meta:
    where: https://github.com/YannDubs/Invariant-Self-Supervised-Learning
    notes: null
    month: 6
    year: 2022
    license: MIT
    is_official: true
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

dissl_resnet50_dNone_e100_m2:
  ssl:
    objective: dissl
    ssl_mode: clustering
    version: 1
    n_negatives: 5120
    n_classes: 16384
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 17e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 66.9
  meta:
    where: https://github.com/YannDubs/Invariant-Self-Supervised-Learning
    notes: null
    month: 6
    year: 2022
    license: MIT
    is_official: true
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

### SimSiam ###
simsiam_rn50_bs256_ep100:
  ssl:
    objective: simsiam
    ssl_mode: distillation
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 9e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 256
    optimizer: lars # larc
    learning_rate: 0.05
    weight_decay: 1e-4
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 68.3
  meta:
    where: https://github.com/facebookresearch/simsiam
    notes: null
    month: 10
    year: 2020
    license: MIT
    is_official: true
  compute:
    n_pus: ???
    pu_type: ???
    time_hours: ???

simsiam_rn50_bs512_ep100:
  ssl:
    objective: simsiam
    ssl_mode: distillation
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 9e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 512
    optimizer: lars # larc
    learning_rate: 0.05
    weight_decay: 1e-4
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 68.1
  meta:
    where: https://github.com/facebookresearch/simsiam
    notes: null
    month: 10
    year: 2020
    license: MIT
    is_official: true
  compute:
    n_pus: ???
    pu_type: ???
    time_hours: ???

### SwAV ###
selav2_rn50_ep400_2x160_4x96:
  ssl:
    objective: sela
    ssl_mode: clustering
    version: 2
    # assuming the following information is same  as swav and dc2
    n_negatives: 8192
    n_classes: 3000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 4096
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x160+4x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 71.8
  meta:
    where: https://github.com/facebookresearch/swav
    notes: null
    month: 6
    year: 2020
    license: MIT
    is_official: true
  compute:
    n_pus: ???
    pu_type: V100
    time_hours: ???

selav2_rn50_ep400_2x224:
  ssl:
    objective: sela
    ssl_mode: clustering
    version: 2
    # assuming the following information is same  as swav and dc2
    n_negatives: 8192
    n_classes: 3000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 4096
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 67.2
  meta:
    where: https://github.com/facebookresearch/swav
    notes: null
    month: 6
    year: 2020
    license: MIT
    is_official: true
  compute:
    n_pus: ???
    pu_type: V100
    time_hours: ???

dc2_rn50_ep800_2x224_6x96:
  ssl:
    objective: deepcluster
    ssl_mode: clustering
    version: 2
    n_negatives: 8192
    n_classes: 3000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 800
    batch_size: 4096
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+6x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 75.2
  meta:
    where: https://github.com/facebookresearch/swav
    notes: null
    month: 6
    year: 2020
    license: MIT
    is_official: true
  compute:
    n_pus: 64
    pu_type: V100
    time_hours: ???

dc2_rn50_ep400_2x160_4x96:
  ssl:
    objective: deepcluster
    ssl_mode: clustering
    version: 2
    n_negatives: 8192
    n_classes: 3000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 4096
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x260+4x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 74.3
  meta:
    where: https://github.com/facebookresearch/swav
    notes: null
    month: 6
    year: 2020
    license: MIT
    is_official: true
  compute:
    n_pus: 64
    pu_type: V100
    time_hours: 21.4

dc2_rn50_ep400_2x224:
  ssl:
    objective: deepcluster
    ssl_mode: clustering
    version: 2
    n_negatives: 8192
    n_classes: 3000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 4096
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 70.2
  meta:
    where: https://github.com/facebookresearch/swav
    notes: null
    month: 6
    year: 2020
    license: MIT
    is_official: true
  compute:
    n_pus: 64
    pu_type: V100
    time_hours: ???

#swav_rn50w4:
#  ssl:
#    objective: swav
#    ssl_mode: clustering
#    version: 1
#    n_negatives: 7680
#    n_classes: 3000
#    is_stopgrad: true
#    is_ema: false
#  model:
#    pred_dim: 128
#    projection_hid_width: 8192
#    projection_hid_depth: 1
#    architecture_exact: resnet50w4
#    architecture: resnet50w4
#    family: resnet
#    patch_size: null
#    projection_arch: mlp
#    n_parameters: 375378176
#    projection_nparameters: 68e6
#  representation:
#    z_dim: 8192
#    z_layer: avgpool
#  optimization:
#    epochs: 400
#    batch_size: 2560
#    optimizer: lars # larc
#    learning_rate: 0.48
#    weight_decay: 1e-6
#    scheduler: cosine
#  data:
#    pretraining_data: ImageNet-1K
#    finetuning_data: null
#    img_size: 224
#    views: 2x224+6x96
#    is_aug_invariant: true
#    augmentations:
#      - crop
#      - color
#      - blur
#      - gray
#      - flip
#      - normalize
#  metrics:
#    top1acc_in1k_official: 77.3
#  meta:
#    where: https://github.com/facebookresearch/swav
#    notes: null
#    month: 2
#    year: 2020
#    license: CC BY-NC 4.0
#    is_official: true
#  compute:
#    n_pus: 64
#    pu_type: V100
#    time_hours: ???

swav_rn50w2:
  ssl:
    objective: swav
    ssl_mode: clustering
    version: 1
    n_negatives: 8192
    n_classes: 3000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 8192
    projection_hid_depth: 1
    architecture_exact: resnet50w2
    architecture: resnet50w2
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 93907072
    projection_nparameters: 35e6
  representation:
    z_dim: 4096
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 4096
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+6x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 77.3
  meta:
    where: https://github.com/facebookresearch/swav
    notes: null
    month: 2
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 128
    pu_type: V100
    time_hours: ???

swav_rn50_ep400_bs256:
  ssl:
    objective: swav
    ssl_mode: clustering
    version: 1
    n_negatives: 4352
    n_classes: 3000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 256
    optimizer: lars # larc
    learning_rate: 0.6
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+6x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 74.3
  meta:
    where: https://github.com/facebookresearch/swav
    notes: null
    month: 2
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 4
    pu_type: V100
    time_hours: 348

swav_rn50_ep400_2x224:
  ssl:
    objective: swav
    ssl_mode: clustering
    version: 1
    n_negatives: 8192
    n_classes: 3000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 4096
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 70.1
  meta:
    where: https://github.com/facebookresearch/swav
    notes: null
    month: 2
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 64
    pu_type: V100
    time_hours: 16.5

swav_rn50_ep400:
  ssl:
    objective: swav
    ssl_mode: clustering
    version: 1
    n_negatives: 8192
    n_classes: 3000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 4096
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+6x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 74.6
  meta:
    where: https://github.com/facebookresearch/swav
    notes: null
    month: 2
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 64
    pu_type: V100
    time_hours: 25

swav_rn50_ep200_bs256:
  ssl:
    objective: swav
    ssl_mode: clustering
    version: 1
    n_negatives: 4352
    n_classes: 3000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 256
    optimizer: lars # larc
    learning_rate: 0.6
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+6x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 72.7
  meta:
    where: https://github.com/facebookresearch/swav
    notes: null
    month: 2
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 4
    pu_type: V100
    time_hours: 348

swav_rn50_ep200:
  ssl:
    objective: swav
    ssl_mode: clustering
    version: 1
    n_negatives: 8192
    n_classes: 3000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 4096
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+6x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 73.9
  meta:
    where: https://github.com/facebookresearch/swav
    notes: null
    month: 2
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 64
    pu_type: V100
    time_hours: 12.5

swav_rn50_ep100:
  ssl:
    objective: swav
    ssl_mode: clustering
    version: 1
    n_negatives: 8192
    n_classes: 3000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 4096
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+6x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 72.1
  meta:
    where: https://github.com/facebookresearch/swav
    notes: null
    month: 2
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 64
    pu_type: V100
    time_hours: 6.25

swav_rn50:
  ssl:
    objective: swav
    ssl_mode: clustering
    version: 1
    n_negatives: 8192
    n_classes: 3000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 800
    batch_size: 4096
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+6x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 75.3
  meta:
    where: https://github.com/facebookresearch/swav
    notes: null
    month: 2
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 64
    pu_type: V100
    time_hours: 50

### VICREG ###
vicreg_rn50w2:
  ssl:
    objective: deepcluster
    ssl_mode: clustering
    version: 1
    n_negatives: 4096
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 8192
    projection_hid_width: 8192
    projection_hid_depth: 2
    architecture_exact: resnet50w2
    architecture: resnet50w2
    family: resnet
    patch_size: null
    projection_arch: linear
    n_parameters: 93907072
    projection_nparameters: 151e6
  representation:
    z_dim: 4096
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 2048
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - gray
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 75.5
  meta:
    where: https://github.com/facebookresearch/vicreg
    notes: null
    month: 1
    year: 2022
    license: MIT
    is_official: true
  compute:
    n_pus: 32
    pu_type: Tesla V100
    time_hours: ???

vicreg_rn50:
  ssl:
    objective: deepcluster
    ssl_mode: clustering
    version: 1
    n_negatives: 4096
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 8192
    projection_hid_width: 8192
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: linear
    n_parameters: 23508032
    projection_nparameters: 151e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 2048
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - gray
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 73.2
  meta:
    where: https://github.com/facebookresearch/vicreg
    notes: null
    month: 1
    year: 2022
    license: MIT
    is_official: true
  compute:
    n_pus: 32
    pu_type: Tesla V100
    time_hours: 110 # see table 14 in their paper


### VISSL ###

pirl_rn50w2_headMLP:
  ssl:
    objective: pirl
    ssl_mode: contrastive
    version: 1
    n_negatives: 32000
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50w2
    architecture: resnet50w2
    family: resnet
    patch_size: null
    projection_arch: linear
    n_parameters: 93907072
    projection_nparameters: 17e6
  representation:
    z_dim: 4096
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 1024
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 0.0001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - solarize
      - photometric
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 70.9
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/projects/PIRL/README.md
    notes: null
    month: 12
    year: 2019
    license: MIT
    is_official: true
  compute:
    n_pus: 32
    pu_type: ???
    time_hours: ???

pirl_rn50w2:
  ssl:
    objective: pirl
    ssl_mode: contrastive
    version: 1
    n_negatives: 32000
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50w2
    architecture: resnet50w2
    family: resnet
    patch_size: null
    projection_arch: linear
    n_parameters: 93907072
    projection_nparameters: 0.6e6
  representation:
    z_dim: 4096
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 1024
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 0.0001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - solarize
      - photometric
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 69.3
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/projects/PIRL/README.md
    notes: null
    month: 12
    year: 2019
    license: MIT
    is_official: true
  compute:
    n_pus: 32
    pu_type: ???
    time_hours: ???

pirl_rn50_ep200_headMLP:
  ssl:
    objective: pirl
    ssl_mode: contrastive
    version: 1
    n_negatives: 32000
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 1024
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 0.0001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - solarize
      - photometric
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 65.8
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/projects/PIRL/README.md
    notes: null
    month: 12
    year: 2019
    license: MIT
    is_official: true
  compute:
    n_pus: 32
    pu_type: ???
    time_hours: ???

pirl_rn50_headMLP:
  ssl:
    objective: pirl
    ssl_mode: contrastive
    version: 1
    n_negatives: 32000
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 800
    batch_size: 1024
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 0.0001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - solarize
      - photometric
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 69.9
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/projects/PIRL/README.md
    notes: null
    month: 12
    year: 2019
    license: MIT
    is_official: true
  compute:
    n_pus: 32
    pu_type: ???
    time_hours: ???

pirl_rn50_ep200:
  ssl:
    objective: pirl
    ssl_mode: contrastive
    version: 1
    n_negatives: 32000
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: linear
    n_parameters: 23508032
    projection_nparameters: 0.4e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 1024
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 0.0001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - solarize
      - photometric
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 62.9
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/projects/PIRL/README.md
    notes: null
    month: 12
    year: 2019
    license: MIT
    is_official: true
  compute:
    n_pus: 32
    pu_type: ???
    time_hours: ???

pirl_rn50:
  ssl:
    objective: pirl
    ssl_mode: contrastive
    version: 1
    n_negatives: 32000
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: linear
    n_parameters: 23508032
    projection_nparameters: 0.4e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 800
    batch_size: 1024
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 0.0001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - solarize
      - photometric
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 64.29
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/projects/PIRL/README.md
    notes: null
    month: 12
    year: 2019
    license: MIT
    is_official: true
  compute:
    n_pus: 32
    pu_type: ???
    time_hours: ???

npidpp_rn50w2:
  ssl:
    objective: npid
    ssl_mode: contrastive
    version: 2 #  npid++ is an improvement of npid
    n_negatives: 32000
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50w2
    architecture: resnet50w2
    family: resnet
    patch_size: null
    projection_arch: linear
    n_parameters: 93907072
    projection_nparameters: 0.5e6
  representation:
    z_dim: 4096
    z_layer: avgpool
  optimization:
    epochs: 800
    batch_size: 1024
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 0.0001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 62.73
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 12
    year: 2019
    license: MIT
    is_official: false
  compute:
    n_pus: 32
    pu_type: ???
    time_hours: ???

npidpp_rn50:
  ssl:
    objective: npid
    ssl_mode: contrastive
    version: 2 #  npid++ is an improvement of npid
    n_negatives: 32000
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: linear
    n_parameters: 23508032
    projection_nparameters: 0.3e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 800
    batch_size: 1024
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 0.0001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 56.68
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 12
    year: 2019
    license: MIT
    is_official: false
  compute:
    n_pus: 32
    pu_type: ???
    time_hours: ???

npid_rn50:
  ssl:
    objective: clusterfit
    ssl_mode: contrastive
    version: 1
    n_negatives: 4096
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: linear
    n_parameters: 23508032
    projection_nparameters: 0.3e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 256
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 0.0001
    scheduler: multistep
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 52.73
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 5
    year: 2018
    license: MIT
    is_official: false
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???

clusterfit_rn50:
  ssl:
    objective: clusterfit
    ssl_mode: clustering
    version: 1
    n_negatives: null
    n_classes: 16000
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: linear
    n_parameters: 23508032
    projection_nparameters: 33e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 105
    batch_size: 256
    optimizer: sgd
    learning_rate: 0.1
    weight_decay: 0.0001
    scheduler: multistep
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 53.63
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 12
    year: 2019
    license: MIT
    is_official: false
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???

jigsaw_rn50:
  ssl:
    objective: jigsaw
    ssl_mode: transform
    version: 1
    n_negatives: null
    n_classes: 2000
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: 9000
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 20e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 105
    batch_size: 256
    optimizer: sgd
    learning_rate: 0.1
    weight_decay: 0.0001
    scheduler: multistep
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 224
    is_aug_invariant: true
    augmentations:
      - crop
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 46.58
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 3
    year: 2016
    license: MIT
    is_official: false
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???

jigsaw_rn50_in22k:
  ssl:
    objective: jigsaw
    ssl_mode: transform
    version: 1
    n_negatives: null
    n_classes: 2000
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: 9000
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 20e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 105
    batch_size: 256
    optimizer: sgd
    learning_rate: 0.1
    weight_decay: 0.0001
    scheduler: multistep
  data:
    pretraining_data: ImageNet-22K
    finetuning_data: null
    img_size: 224
    views: 224
    is_aug_invariant: true
    augmentations:
      - crop
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 53.09
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 3
    year: 2016
    license: MIT
    is_official: false
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???

simclr_rn101_ep100:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 8192
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet101
    architecture: resnet101
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 42500160
    projection_nparameters: 4e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 4096
    optimizer: lars #larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 62.76
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 2
    year: 2020
    license: MIT
    is_official: false
  compute:
    n_pus: 64
    pu_type: ???
    time_hours: ???

simclr_rn101:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 8192
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet101
    architecture: resnet101
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 42500160
    projection_nparameters: 68e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 4096
    optimizer: lars #larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 71.56
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 2
    year: 2020
    license: MIT
    is_official: false
  compute:
    n_pus: 64
    pu_type: ???
    time_hours: ???

simclr_rn50w4:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 8192
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 8192
    projection_hid_depth: 1
    architecture_exact: resnet50w4
    architecture: resnet50w4
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 375378176
    projection_nparameters: 68e6
  representation:
    z_dim: 8192
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 4096
    optimizer: lars #larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 71.61
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 2
    year: 2020
    license: MIT
    is_official: false
  compute:
    n_pus: 64
    pu_type: ???
    time_hours: ???

simclr_rn50w2_ep100:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 8192
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50w2
    architecture: resnet50w2
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 93907072
    projection_nparameters: 17e6
  representation:
    z_dim: 4096
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 4096
    optimizer: lars #larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 69.82
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 2
    year: 2020
    license: MIT
    is_official: false
  compute:
    n_pus: 64
    pu_type: ???
    time_hours: ???

simclr_rn50w2:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 8192
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50w2
    architecture: resnet50w2
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 93907072
    projection_nparameters: 17e6
  representation:
    z_dim: 4096
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 4096
    optimizer: lars #larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 73.84
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 2
    year: 2020
    license: MIT
    is_official: false
  compute:
    n_pus: 64
    pu_type: ???
    time_hours: ???

simclr_rn50_bs4096_ep100:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 8192
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 4e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 800
    batch_size: 4096
    optimizer: lars #larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 64.4
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 2
    year: 2020
    license: MIT
    is_official: false
  compute:
    n_pus: 64
    pu_type: ???
    time_hours: ???

simclr_rn50_ep800:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 8192
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 4e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 800
    batch_size: 4096
    optimizer: lars #larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 69.68
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 2
    year: 2020
    license: MIT
    is_official: false
  compute:
    n_pus: 64
    pu_type: ???
    time_hours: ???

simclr_rn50_ep400:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 8192
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 4e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 4096
    optimizer: lars #larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 67.71
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 2
    year: 2020
    license: MIT
    is_official: false
  compute:
    n_pus: 64
    pu_type: ???
    time_hours: ???

simclr_rn50_ep200:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 8192
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 4e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 4096
    optimizer: lars #larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 66.61
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 2
    year: 2020
    license: MIT
    is_official: false
  compute:
    n_pus: 64
    pu_type: ???
    time_hours: ???

simclr_rn50:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 8192
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 4e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 4096
    optimizer: lars #larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 68.8
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 2
    year: 2020
    license: MIT
    is_official: false
  compute:
    n_pus: 64
    pu_type: ???
    time_hours: ???

rotnet_rn50_in22k:
  ssl:
    objective: rotnet
    ssl_mode: transform # predict the transformation
    version: 1
    n_negatives: null
    n_classes: 4
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: linear
    n_parameters: 23508032
    projection_nparameters: 0.1e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 105
    batch_size: 256
    optimizer: sgd
    learning_rate: 0.1
    weight_decay: 0.0001
    scheduler: multistep
  data:
    pretraining_data: ImageNet-22K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 54.89
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 3
    year: 2018
    license: MIT
    is_official: false
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???

rotnet_rn50_in1k:
  ssl:
    objective: rotnet
    ssl_mode: transform # predict the transformation
    version: 1
    n_negatives: null
    n_classes: 4
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: linear
    n_parameters: 23508032
    projection_nparameters: 0.1e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 105
    batch_size: 256
    optimizer: sgd
    learning_rate: 0.1
    weight_decay: 0.0001
    scheduler: multistep
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 48.2
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 3
    year: 2018
    license: MIT
    is_official: false
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???

mocov2_rn50_vissl:
  ssl:
    objective: moco
    ssl_mode: contrastive
    version: 2
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 4e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 256
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 0.0001
    scheduler: multistep
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - grayscale
      - blur
      - normalize
  metrics:
    top1acc_in1k_official: 66.4
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 9
    year: 2020
    license: MIT
    is_official: false
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???

barlow_rn50_ep300:
  ssl:
    objective: barlow_twins
    ssl_mode: contrastive
    version: 1
    n_negatives: 4095
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 8192
    projection_hid_width: 8192
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 151e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 300
    batch_size: 2048
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1.5e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - grayscale
      - blur
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 70.75
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 3
    year: 2021
    license: MIT
    is_official: false
  compute:
    n_pus: 32
    pu_type: ???
    time_hours: ???

barlow_rn50:
  ssl:
    objective: barlow_twins
    ssl_mode: contrastive
    version: 1
    n_negatives: 4095
    n_classes: null
    is_stopgrad: false
    is_ema: flase
  model:
    pred_dim: 8192
    projection_hid_width: 8192
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 151e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 2048
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - grayscale
      - blur
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 71.8
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 3
    year: 2021
    license: MIT
    is_official: false
  compute:
    n_pus: 32
    pu_type: ???
    time_hours: ???

### BYOL ###
byol_rn50_bs4096:
  ssl:
    objective: byol
    ssl_mode: distillation
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 12e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 4096
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1.5e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - grayscale
      - blur
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 74.4
  meta:
    where: https://github.com/deepmind/deepmind-research/tree/master/byol
    notes: the weights are from the official implementation but converted tf -> torch. The main difference in the resnet50 is how padding is done.
    month: 6
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 512
    pu_type: Cloud TPU v3
    time_hours: 8

byol_rn50_bs2048:
  ssl:
    objective: byol
    ssl_mode: distillation
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 12e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 2048
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1.5e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - grayscale
      - blur
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 72.4
  meta:
    where: https://github.com/deepmind/deepmind-research/tree/master/byol
    notes: the weights are from the official implementation but converted tf -> torch. The main difference in the resnet50 is how padding is done.
    month: 6
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 512
    pu_type: Cloud TPU v3
    time_hours: ???

byol_rn50_bs1024:
  ssl:
    objective: byol
    ssl_mode: distillation
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 12e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 1024
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1.5e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - grayscale
      - blur
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 72.2
  meta:
    where: https://github.com/deepmind/deepmind-research/tree/master/byol
    notes: the weights are from the official implementation but converted tf -> torch. The main difference in the resnet50 is how padding is done.
    month: 6
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 512
    pu_type: Cloud TPU v3
    time_hours: ???

byol_rn50_bs512:
  ssl:
    objective: byol
    ssl_mode: distillation
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 12e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 512
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1.5e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - grayscale
      - blur
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 72.2
  meta:
    where: https://github.com/deepmind/deepmind-research/tree/master/byol
    notes: the weights are from the official implementation but converted tf -> torch. The main difference in the resnet50 is how padding is done.
    month: 6
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 512
    pu_type: Cloud TPU v3
    time_hours: ???

byol_rn50_bs256:
  ssl:
    objective: byol
    ssl_mode: distillation
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 12e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 256
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1.5e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - grayscale
      - blur
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 71.8
  meta:
    where: https://github.com/deepmind/deepmind-research/tree/master/byol
    notes: the weights are from the official implementation but converted tf -> torch. The main difference in the resnet50 is how padding is done.
    month: 6
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 512
    pu_type: Cloud TPU v3
    time_hours: ???

byol_rn50_bs128:
  ssl:
    objective: byol
    ssl_mode: distillation
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 12e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 128
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1.5e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - grayscale
      - blur
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 69.6
  meta:
    where: https://github.com/deepmind/deepmind-research/tree/master/byol
    notes: the weights are from the official implementation but converted tf -> torch. The main difference in the resnet50 is how padding is done.
    month: 6
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 512
    pu_type: Cloud TPU v3
    time_hours: ???

byol_rn50_bs64:
  ssl:
    objective: byol
    ssl_mode: distillation
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 12e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 64
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1.5e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - grayscale
      - blur
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 59.7
  meta:
    where: https://github.com/deepmind/deepmind-research/tree/master/byol
    notes: the weights are from the official implementation but converted tf -> torch. The main difference in the resnet50 is how padding is done.
    month: 6
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 512
    pu_type: Cloud TPU v3
    time_hours: ???

byol_rn50_augCropBlur:
  ssl:
    objective: byol
    ssl_mode: distillation
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 12e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 4096
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1.5e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - blur
      - normalize
  metrics:
    top1acc_in1k_official: 61.1
  meta:
    where: https://github.com/deepmind/deepmind-research/tree/master/byol
    notes: the weights are from the official implementation but converted tf -> torch. The main difference in the resnet50 is how padding is done.
    month: 6
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 512
    pu_type: Cloud TPU v3
    time_hours: ???

byol_rn50_augCropColor:
  ssl:
    objective: byol
    ssl_mode: distillation
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 12e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 4096
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1.5e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - solarize # I'm assuming that solarize is considered as color (couldn't find inofmration)
      - normalize
  metrics:
    top1acc_in1k_official: 70.7
  meta:
    where: https://github.com/deepmind/deepmind-research/tree/master/byol
    notes: the weights are from the official implementation but converted tf -> torch. The main difference in the resnet50 is how padding is done.
    month: 6
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 512
    pu_type: Cloud TPU v3
    time_hours: ???

byol_rn50_augCrop:
  ssl:
    objective: byol
    ssl_mode: distillation
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 12e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 4096
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1.5e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 59.4
  meta:
    where: https://github.com/deepmind/deepmind-research/tree/master/byol
    notes: the weights are from the official implementation but converted tf -> torch. The main difference in the resnet50 is how padding is done.
    month: 6
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 512
    pu_type: Cloud TPU v3
    time_hours: ???

byol_rn50_augNocolor:
  ssl:
    objective: byol
    ssl_mode: distillation
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 12e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 4096
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1.5e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - grayscale
      - blur
      - normalize
  metrics:
    top1acc_in1k_official: 59.4
  meta:
    where: https://github.com/deepmind/deepmind-research/tree/master/byol
    notes: the weights are from the official implementation but converted tf -> torch. The main difference in the resnet50 is how padding is done.
    month: 6
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 512
    pu_type: Cloud TPU v3
    time_hours: ???

byol_rn50_augNogray:
  ssl:
    objective: byol
    ssl_mode: distillation
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection_arch: mlp
    n_parameters: 23508032
    projection_nparameters: 12e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 4096
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1.5e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - solarize
      - blur
      - normalize
  metrics:
    top1acc_in1k_official: 70.3
  meta:
    where: https://github.com/deepmind/deepmind-research/tree/master/byol
    notes: the weights are from the official implementation but converted tf -> torch. The main difference in the resnet50 is how padding is done.
    month: 6
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
  compute:
    n_pus: 512
    pu_type: Cloud TPU v3
    time_hours: ???

######################## HERE ARE ALL SUPERVISED MODELS ########################
### TIMM ###
sup_vitS16_dino_extractB:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: vits16 cls+avg
    architecture: vits
    patch_size: 16
  representation:
    z_dim: 768
    z_layer: cls+avg
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

sup_vitS16_dino:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: vits16 4xcls
    architecture: vits
    patch_size: 16
  representation:
    z_dim: 1536
    z_layer: 4xcls
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: ???
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

sup_vitS16:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: vits16 cls
    architecture: vits
    patch_size: 16
  representation:
    z_dim: 384
    z_layer: cls
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: 81.396
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

sup_vitL16:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: vitl16 cls
    architecture: vitl
    patch_size: 16
  representation:
    z_dim: 1024
    z_layer: cls
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: 85.844
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

sup_vitL14:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: vitl14 cls
    architecture: vitl
    patch_size: 14
  representation:
    z_dim: 1024
    z_layer: cls
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

sup_vitH14:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: vith14 cls
    architecture: vith
    patch_size: 14
  representation:
    z_dim: 1280
    z_layer: cls
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

sup_vitB32:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: vitb32 cls
    architecture: vitb
    patch_size: 32
  representation:
    z_dim: 768
    z_layer: cls
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: 80.724
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

sup_vitB16_dino_extractS:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: vitb16 4xcls
    architecture: vitb
    patch_size: 16
  representation:
    z_dim: 3072
    z_layer: 4xcls
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

sup_vitB16_dino:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: vitb16 cls+avg
    architecture: vitb
    patch_size: 16
  representation:
    z_dim: 1536
    z_layer: cls+avg
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

sup_vitB16:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: vitb16 cls
    architecture: vitb
    patch_size: 16
  representation:
    z_dim: 768
    z_layer: cls
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: 84.530
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

sup_vitB8_dino:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: vitb8 cls+avg
    architecture: vitb
    patch_size: 8
  representation:
    z_dim: 1536
    z_layer: cls+avg
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

sup_vitB8:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: vitb8 cls
    architecture: vitb
    patch_size: 8
  representation:
    z_dim: 768
    z_layer: cls
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: 85.790
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

### Torchvision ###
sup_rn50:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: resnet50
    architecture: resnet50
    patch_size: null
  representation:
    z_dim: 2048
    z_layer: avgpool
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: 76.13
  meta:
    where: https://github.com/pytorch/vision
    license: BSD 3-Clause
    is_official: false

sup_rn50w2:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: wide_resnet50_2
    architecture: resnet50w2
    patch_size: null
  representation:
    z_dim: 2048
    z_layer: avgpool
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: 78.468
  meta:
    where: https://github.com/pytorch/vision
    license: BSD 3-Clause
    is_official: false
    notes: this is not exactly correct, we are using wide_resnet50_2 (Wide_ResNet101_2_Weights.IMAGENET1K_V2) instead of resnet50w2 because I couldn't find parameters for the latter

sup_rn101:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: resnet101
    architecture: resnet101
    patch_size: null
  representation:
    z_dim: 2048
    z_layer: avgpool
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: 78.312
  meta:
    where: https://github.com/pytorch/vision
    license: BSD 3-Clause
    is_official: false
