# TODO document each field
# learning_rate : say that for bs 256

### BEIT ###
beitv2_vitB16_pt1k_extractB:
  ssl:
    objective: beit
    ssl_mode: generative
    version: 2
    n_negatives: null
    n_classes: null # it uses vocabulary size of 8192, but that's not global classes
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: null # 2048 for proj and 4096 for pred
    projection_hid_depth: null # 3 for proj and 3 for pred
    architecture_exact: beitb16 cls+avg
    architecture: vitb
    family: vit
    patch_size: 16
    projection1_arch: null
    projection2_arch: null
    projection_same: null
    n_parameters: 85761984
    projection_nparameters: null
  representation:
    z_dim: 1536
    z_layer: cls+avg
  optimization:
    epochs: 1600
    batch_size: 2048
    optimizer: adam
    learning_rate:  1.875e-4
    weight_decay: 0.05
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 224
    is_aug_invariant: False
    augmentations:
      - crop
      - color
      - masking
      - normalize
  metrics:
    top1acc_in1k_official: null # 80.1 is when using 300 epochs
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/microsoft/unilm/tree/master/beit2
    notes: the architecture is slightly different than vit because they use relative positional embeddings
    month: 8
    year: 2022
    license: MIT
    is_official: true
    is_industry: True
    is_our_extract: false
  compute:
    n_pus: 16
    pu_type: V100
    time_hours: ???

beitv2_vitB16_pt1k:
  ssl:
    objective: beit
    ssl_mode: generative
    version: 2
    n_negatives: null
    n_classes: null # it uses vocabulary size of 8192, but that's not global classes
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: null # 2048 for proj and 4096 for pred
    projection_hid_depth: null # 3 for proj and 3 for pred
    architecture_exact: beitb16 cls
    architecture: vitb
    family: vit
    patch_size: 16
    projection1_arch: null
    projection2_arch: null
    projection_same: null
    n_parameters: 85761984
    projection_nparameters: null
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 1600
    batch_size: 2048
    optimizer: adam
    learning_rate:  1.875e-4
    weight_decay: 0.05
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 224
    is_aug_invariant: False
    augmentations:
      - crop
      - color
      - masking
      - normalize
  metrics:
    top1acc_in1k_official: null # 80.1 is when using 300 epochs
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/microsoft/unilm/tree/master/beit2
    notes: the architecture is slightly different than vit because they use relative positional embeddings
    month: 8
    year: 2022
    license: MIT
    is_official: true
    is_industry: True
    is_our_extract: true
  compute:
    n_pus: 16
    pu_type: V100
    time_hours: ???


beitv2_vitL16_pt1k:
  ssl:
    objective: beit
    ssl_mode: generative
    version: 2
    n_negatives: null
    n_classes: null # it uses vocabulary size of 8192, but that's not global classes
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: null # 2048 for proj and 4096 for pred
    projection_hid_depth: null # 3 for proj and 3 for pred
    architecture_exact: beitl16 cls
    architecture: vitl
    family: vit
    patch_size: 16
    projection1_arch: null
    projection2_arch: null
    projection_same: null
    n_parameters: 303301632
    projection_nparameters: null
  representation:
    z_dim: 1024
    z_layer: cls
  optimization:
    epochs: 1600
    batch_size: 2048
    optimizer: adam
    learning_rate:  1.875e-4
    weight_decay: 0.05
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 224
    is_aug_invariant: False
    augmentations:
      - crop
      - color
      - masking
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/microsoft/unilm/tree/master/beit2
    notes: the architecture is slightly different than vit because they use relative positional embeddings
    month: 8
    year: 2022
    license: MIT
    is_official: true
    is_industry: True
    is_our_extract: False
  compute:
    n_pus: 16
    pu_type: V100
    time_hours: ???

beitv2_vitB16_pt1k_ep300:
  ssl:
    objective: beit
    ssl_mode: generative
    version: 2
    n_negatives: null
    n_classes: null # it uses vocabulary size of 8192, but that's not global classes
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: null # 2048 for proj and 4096 for pred
    projection_hid_depth: null # 3 for proj and 3 for pred
    architecture_exact: beitb16 cls
    architecture: vitb
    family: vit
    patch_size: 16
    projection1_arch: null
    projection2_arch: null
    projection_same: null
    n_parameters: 85761984
    projection_nparameters: null
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 300
    batch_size: 2048
    optimizer: adam
    learning_rate:  1.875e-4
    weight_decay: 0.05
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 224
    is_aug_invariant: False
    augmentations:
      - crop
      - color
      - masking
      - normalize
  metrics:
    top1acc_in1k_official: null # 80.1 is when using cls+avg and 300 epochs
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/microsoft/unilm/tree/master/beit2
    notes: the architecture is slightly different than vit because they use relative positional embeddings
    month: 8
    year: 2022
    license: MIT
    is_official: true
    is_industry: True
    is_our_extract: true
  compute:
    n_pus: 16
    pu_type: V100
    time_hours: ???

beit_vitL16_pt22k:
  ssl:
    objective: beit
    ssl_mode: generative
    version: 1
    n_negatives: null
    n_classes: null # it uses vocabulary size of 8192, but that's not global classes
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: null # 2048 for proj and 4096 for pred
    projection_hid_depth: null # 3 for proj and 3 for pred
    architecture_exact: beitl16 cls
    architecture: vitl
    family: vit
    patch_size: 16
    projection1_arch: null
    projection2_arch: null
    projection_same: null
    n_parameters: 303301632
    projection_nparameters: null
  representation:
    z_dim: 1024
    z_layer: cls
  optimization:
    epochs: 800
    batch_size: 2048
    optimizer: adam
    learning_rate:  1.875e-4
    weight_decay: 0.05
    scheduler: cosine
  data:
    pretraining_data: ImageNet-22K
    finetuning_data: null
    img_size: 224
    views: 224
    is_aug_invariant: False
    augmentations:
      - crop
      - masking
      - flip
      - color
      - normalize
  metrics:
    top1acc_in1k_official: null # 73.5 is when using cls+avg
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/microsoft/unilm/tree/master/beit
    notes: the architecture is slightly different than vit because they use relative positional embeddings
    month: 6
    year: 2021
    license: MIT
    is_official: true
    is_industry: True
    is_our_extract: true
  compute:
    n_pus: 16
    pu_type: V100
    time_hours: 120

beit_vitB16_pt22k:
  ssl:
    objective: beit
    ssl_mode: generative
    version: 1
    n_negatives: null
    n_classes: null # it uses vocabulary size of 8192, but that's not global classes
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: null # 2048 for proj and 4096 for pred
    projection_hid_depth: null # 3 for proj and 3 for pred
    architecture_exact: beitb16 cls
    architecture: vitb
    family: vit
    patch_size: 16
    projection1_arch: null
    projection2_arch: null
    projection_same: null
    n_parameters: 85761984
    projection_nparameters: null
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 800
    batch_size: 2048
    optimizer: adam
    learning_rate:  1.875e-4
    weight_decay: 0.05
    scheduler: cosine
  data:
    pretraining_data: ImageNet-22K
    finetuning_data: null
    img_size: 224
    views: 224
    is_aug_invariant: False
    augmentations:
      - crop
      - masking
      - flip
      - color
      - normalize
  metrics:
    top1acc_in1k_official: null # 56.7 is for cls+avg
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/microsoft/unilm/tree/master/beit
    notes: the architecture is slightly different than vit because they use relative positional embeddings
    month: 6
    year: 2021
    license: MIT
    is_official: true
    is_industry: True
    is_our_extract: true
  compute:
    n_pus: 16
    pu_type: V100
    time_hours: 120


### MMSelfSup ###
deepcluster_rn50_bs512_ep200_mmselfsup:
  ssl:
    objective: deepcluster
    ssl_mode: clustering
    version: 1
    n_negatives: null
    n_classes: 10000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 4456448
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 512
    optimizer: sgd
    learning_rate: 0.1
    weight_decay: 1e-5
    scheduler: multistep
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 224
    is_aug_invariant: true
    augmentations:
      - crop
      - gray
      - color
      - flip
      - rotate
      - normalize
  metrics:
    top1acc_in1k_official: 46.92
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/open-mmlab/mmselfsup
    notes: null
    month: 7
    year: 2018
    license: Apache 2.0
    is_official: false
    is_industry: True
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: V100
    time_hours: ???

simclr_rn50_bs256_ep200_mmselfsup:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 512
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 4456448
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 256
    optimizer: lars #larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 62.56
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/open-mmlab/mmselfsup
    notes: null
    month: 2
    year: 2020
    license: Apache 2.0
    is_official: false
    is_industry: True
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???

simsiam_rn50_bs256_ep200_mmselfsup:
  ssl:
    objective: simsiam
    ssl_mode: siamese
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: 2048
    projection_hid_depth: 5
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 17e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 256
    optimizer: sgd # larc
    learning_rate: 0.05
    weight_decay: 1e-4
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - gray
      - color
      - flip
      - blur
      - normalize
  metrics:
    top1acc_in1k_official: 69.84
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/open-mmlab/mmselfsup
    notes: null
    month: 10
    year: 2020
    license: Apache 2.0
    is_official: false
    is_industry: True
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???

densecl_rn50_200ep_mmselfsup:
  ssl:
    objective: densecl
    ssl_mode: hierarchical
    version: 1
    n_negatives: 512
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 256
    projection_hid_width: 512
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 2e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 256
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 1e-4
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: false # it's equivariant
    augmentations:
      - crop
      - gray
      - color
      - flip
      - blur
      - normalize
  metrics:
    top1acc_in1k_official: 63.34
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/open-mmlab/mmselfsup
    notes: null
    month: 9
    year: 2020
    license: Apache 2.0
    is_official: false
    is_industry: False
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???

odc_rn50_440ep_mmselfsup:
  ssl:
    objective: odc
    ssl_mode: clustering
    version: 1
    n_negatives: null
    n_classes: 10000
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 256
    projection_hid_width: 512
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: null
    projection_same: null
    n_parameters: 23508032
    projection_nparameters: 2e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 440
    batch_size: 512
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 1e-5
    scheduler: multistep
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - gray
      - color
      - flip
      - rotate
      - normalize
  metrics:
    top1acc_in1k_official: 53.42
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/open-mmlab/mmselfsup
    notes: null
    month: 6
    year: 2020
    license: Apache 2.0
    is_official: false
    is_industry: False
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???

relativeloc_rn50_70ep_mmselfsup:
  ssl:
    objective: locnet
    ssl_mode: transform # predict the transformation
    version: 1
    n_negatives: null
    n_classes: 8
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 4096
    projection_hid_depth: 3
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: null
    projection_same: null
    n_parameters: 23508032
    projection_nparameters: 50e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 70
    batch_size: 512
    optimizer: sgd
    learning_rate: 0.1
    weight_decay: 1e-4
    scheduler: multistep
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: 39.65
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/open-mmlab/mmselfsup
    notes: null
    month: 5
    year: 2015
    license: Apache 2.0
    is_official: false
    is_industry: False
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???



##### Pycontrast #####
infomin_rn50_800ep:
  ssl:
    objective: moco
    ssl_mode: contrastive
    version: 2
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 800
    batch_size: 256
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 1e-4
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - gray
      - blur
      - autoaugment
      - jigsaw
      - normalize
  metrics:
    top1acc_in1k_official: 73.0
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/HobbitLong/PyContrast
    notes: this is simply mocov2 with autoaugment and jigsaw augment
    month: 5
    year: 2020
    license: BSD 3-Clause
    is_official: true
    is_industry: False
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???

infomin_rn50_200ep:
  ssl:
    objective: moco
    ssl_mode: contrastive
    version: 2
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 256
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 1e-4
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - gray
      - blur
      - autoaugment
      - jigsaw
      - normalize
  metrics:
    top1acc_in1k_official: 70.1
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/HobbitLong/PyContrast
    notes: this is simply mocov2 with autoaugment and jigsaw augment
    month: 5
    year: 2020
    license: BSD 3-Clause
    is_official: true
    is_industry: False
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???

##### MOCO #####
mocov2_rn50_ep800:
  ssl:
    objective: moco
    ssl_mode: contrastive
    version: 2
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 800
    batch_size: 256
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 1e-4
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - gray
      - blur
      - normalize
  metrics:
    top1acc_in1k_official: 71.1
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/moco
    notes: null
    month: 3
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: V100
    time_hours: 212

mocov2_rn50_ep200:
  ssl:
    objective: moco
    ssl_mode: contrastive
    version: 2
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 256
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 1e-4
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - gray
      - blur
      - normalize
  metrics:
    top1acc_in1k_official: 67.7
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/moco
    notes: null
    month: 3
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: V100
    time_hours: 53

mocov1_rn50_ep200:
  ssl:
    objective: moco
    ssl_mode: contrastive
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 128
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: linear
    projection2_arch: linear
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 0.3e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 256
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 1e-4
    scheduler: multistep
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: 60.6
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/moco
    notes: null
    month: 11
    year: 2019
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: V100
    time_hours: ???

##### MOCOV3 #####
mocov3_vitB_ep300:
  ssl:
    objective: moco
    ssl_mode: contrastive
    version: 3
    n_negatives: 8192
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 5
    architecture_exact: vitb16 cls
    architecture: vitb
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 85798656
    projection_nparameters: 28e6
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 300
    batch_size: 4096
    optimizer: adamw
    learning_rate: 1.5e-4
    weight_decay: 0.1
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - blur
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: 76.7
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/moco-v3
    notes: null
    month: 4
    year: 2021
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 64
    pu_type: Volta
    time_hours: ???

mocov3_vitS_ep300:
  ssl:
    objective: moco
    ssl_mode: contrastive
    version: 3
    n_negatives: 8192
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 5
    architecture_exact: vits16 cls
    architecture: vits
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 21665664
    projection_nparameters: 28e6
  representation:
    z_dim: 384
    z_layer: cls
  optimization:
    epochs: 300
    batch_size: 4096
    optimizer: adamw
    learning_rate: 1.5e-4
    weight_decay: 0.1
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - blur
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: 73.2
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/moco-v3
    notes: moco uses 12 heads instead of the standard 6
    month: 4
    year: 2021
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 64
    pu_type: Volta
    time_hours: ???

mocov3_rn50_ep1000:
  ssl:
    objective: moco
    ssl_mode: contrastive
    version: 3
    n_negatives: 8192
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 5
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 28e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 4096
    optimizer: lars
    learning_rate:  0.6
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - blur
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: 74.6
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/moco-v3
    notes: null
    month: 4
    year: 2021
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 16
    pu_type: Volta
    time_hours: ???

mocov3_rn50_ep300:
  ssl:
    objective: moco
    ssl_mode: contrastive
    version: 3
    n_negatives: 8192
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 5
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 28e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 300
    batch_size: 4096
    optimizer: lars
    learning_rate:  0.6
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - blur
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: 72.8
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/moco-v3
    notes: null
    month: 4
    year: 2021
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 16
    pu_type: Volta
    time_hours: ???

mocov3_rn50_ep100:
  ssl:
    objective: moco
    ssl_mode: contrastive
    version: 3
    n_negatives: 8192
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 5
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 28e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 4096
    optimizer: lars
    learning_rate:  0.6
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - blur
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: 68.9
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/moco-v3
    notes: null
    month: 4
    year: 2021
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 16
    pu_type: Volta
    time_hours: ???


##### MSN #####
msn_vitl7_ep200:
  ssl:
    objective: msn
    ssl_mode: clustering
    version: 1
    n_negatives: 12288
    n_classes: 1024
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitl7 cls
    architecture: vitl
    family: vit
    patch_size: 7
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 303301632
    projection_nparameters: 13e6
  representation:
    z_dim: 1024
    z_layer: cls
  optimization:
    epochs: 200
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.001
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - masking
      - flip
      - color
      - blur
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official:  75.1
    top1acc_in1k-c5_official:  72.1
  meta:
    where: https://github.com/facebookresearch/msn
    notes: null
    month: 4
    year: 2022
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: ???
    pu_type: ???
    time_hours: ???

msn_vitl7_ep200_extractb:
  ssl:
    objective: msn
    ssl_mode: clustering
    version: 1
    n_negatives: 12288
    n_classes: 1024
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitl7 cls+avg
    architecture: vitl
    family: vit
    patch_size: 7
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 303301632
    projection_nparameters: 13e6
  representation:
    z_dim: 2048
    z_layer: cls+avg
  optimization:
    epochs: 200
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.001
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - masking
      - flip
      - color
      - blur
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/msn
    notes: null
    month: 4
    year: 2022
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: true
  compute:
    n_pus: ???
    pu_type: ???
    time_hours: ???

msn_vitl7_ep200_extracts:
  ssl:
    objective: msn
    ssl_mode: clustering
    version: 1
    n_negatives: 12288
    n_classes: 1024
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitl7 4xcls
    architecture: vitl
    family: vit
    patch_size: 7
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 303301632
    projection_nparameters: 13e6
  representation:
    z_dim: 4096
    z_layer: 4xcls
  optimization:
    epochs: 200
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.001
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - masking
      - flip
      - color
      - blur
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/msn
    notes: null
    month: 4
    year: 2022
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: true
  compute:
    n_pus: ???
    pu_type: ???
    time_hours: ???

msn_vitl16_ep600:
  ssl:
    objective: msn
    ssl_mode: clustering
    version: 1
    n_negatives: 12288
    n_classes: 1024
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitl16 cls
    architecture: vitl
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 303301632
    projection_nparameters: 13e6
  representation:
    z_dim: 1024
    z_layer: cls
  optimization:
    epochs: 300
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.001
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - masking
      - flip
      - color
      - blur
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: 80.7
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/msn
    notes: null
    month: 4
    year: 2022
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: ???
    pu_type: ???
    time_hours: ???

msn_vitb4_ep300:
  ssl:
    objective: msn
    ssl_mode: clustering
    version: 1
    n_negatives: 12288
    n_classes: 1024
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitb4 cls
    architecture: vitb
    family: vit
    patch_size: 4
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 85798656
    projection_nparameters: 13e6
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 300
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.001
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - masking
      - flip
      - color
      - blur
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: 75.7
    top1acc_in1k-c5_official: 72.4
  meta:
    where: https://github.com/facebookresearch/msn
    notes: null
    month: 4
    year: 2022
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: ???
    pu_type: ???
    time_hours: ???

msn_vitb4_ep300_extractb:
  ssl:
    objective: msn
    ssl_mode: clustering
    version: 1
    n_negatives: 12288
    n_classes: 1024
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitb4 cls+avg
    architecture: vitb
    family: vit
    patch_size: 4
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 85798656
    projection_nparameters: 13e6
  representation:
    z_dim: 1536
    z_layer: cls+avg
  optimization:
    epochs: 300
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.001
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - masking
      - flip
      - color
      - blur
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/msn
    notes: null
    month: 4
    year: 2022
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: true
  compute:
    n_pus: ???
    pu_type: ???
    time_hours: ???

msn_vitb4_ep300_extracts:
  ssl:
    objective: msn
    ssl_mode: clustering
    version: 1
    n_negatives: 12288
    n_classes: 1024
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitb4 4xcls
    architecture: vitb
    family: vit
    patch_size: 4
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 85798656
    projection_nparameters: 13e6
  representation:
    z_dim: 3072
    z_layer: 4xcls
  optimization:
    epochs: 300
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.001
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - masking
      - flip
      - color
      - blur
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/msn
    notes: null
    month: 4
    year: 2022
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: true
  compute:
    n_pus: ???
    pu_type: ???
    time_hours: ???

msn_vitb16_ep600:
  ssl:
    objective: msn
    ssl_mode: clustering
    version: 1
    n_negatives: 12288
    n_classes: 1024
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitb16 cls
    architecture: vitb
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 85798656
    projection_nparameters: 13e6
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 600
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.001
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - masking
      - flip
      - color
      - blur
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official:  65.5
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/msn
    notes: null
    month: 4
    year: 2022
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 16
    pu_type: ???
    time_hours: ???

msn_vits16_ep800:
  ssl:
    objective: msn
    ssl_mode: clustering
    version: 1
    n_negatives: 12288
    n_classes: 1024
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vits16 cls
    architecture: vits
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 21665664
    projection_nparameters: 13e6
  representation:
    z_dim: 384
    z_layer: cls
  optimization:
    epochs: 800
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.001
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - masking
      - flip
      - color
      - blur
      - gray
      - normalize
  metrics:
    top1acc_in1k_official: 76.9
    top1acc_in1k-1%_official: 67.2
    top1acc_in1k-c5_official: 62.8
  meta:
    where: https://github.com/facebookresearch/msn
    notes: null
    month: 4
    year: 2022
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 16
    pu_type: ???
    time_hours: ???


##### MAE #####
mae_vitH14:
  ssl:
    objective: mae
    ssl_mode: generative
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: null # 2048 for proj and 4096 for pred
    projection_hid_depth: null # 3 for proj and 3 for pred
    architecture_exact: vith14 cls
    architecture: vith
    family: vit
    patch_size: 14
    projection1_arch: null
    projection2_arch: null
    projection_same: null
    n_parameters: 630764800
    projection_nparameters: null
  representation:
    z_dim: 1280
    z_layer: cls
  optimization:
    epochs: 1600
    batch_size: 4096
    optimizer: adamw
    learning_rate:  1.5e-4
    weight_decay: 0.05
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 224
    is_aug_invariant: False
    augmentations:
      - crop
      - masking
      - normalize
  metrics:
    top1acc_in1k_official: 76.6
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/mae
    notes: null
    month: 11
    year: 2021
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 128
    pu_type: TPU v3
    time_hours: 78

mae_vitL16:
  ssl:
    objective: mae
    ssl_mode: generative
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: null # 2048 for proj and 4096 for pred
    projection_hid_depth: null # 3 for proj and 3 for pred
    architecture_exact: vitl16 cls
    architecture: vitl
    family: vit
    patch_size: 16
    projection1_arch: null
    projection2_arch: null
    projection_same: null
    n_parameters: 303301632
    projection_nparameters: null
  representation:
    z_dim: 1024
    z_layer: cls
  optimization:
    epochs: 1600
    batch_size: 4096
    optimizer: adamw
    learning_rate:  1.5e-4
    weight_decay: 0.05
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 224
    is_aug_invariant: False
    augmentations:
      - crop
      - masking
      - normalize
  metrics:
    top1acc_in1k_official: 75.8
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/mae
    notes: null
    month: 11
    year: 2021
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 128
    pu_type: TPU v3
    time_hours: 31

mae_vitB16:
  ssl:
    objective: mae
    ssl_mode: generative
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: null # 2048 for proj and 4096 for pred
    projection_hid_depth: null # 3 for proj and 3 for pred
    architecture_exact: vitb16 cls
    architecture: vitb
    family: vit
    patch_size: 16
    projection1_arch: null
    projection2_arch: null
    projection_same: null
    n_parameters: 85798656
    projection_nparameters: null
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 1600
    batch_size: 4096
    optimizer: adamw
    learning_rate:  1.5e-4
    weight_decay: 0.05
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 224
    is_aug_invariant: False
    augmentations:
      - crop
      - masking
      - normalize
  metrics:
    top1acc_in1k_official: 68.0
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/mae
    notes: null
    month: 11
    year: 2021
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 128
    pu_type: TPU v3
    time_hours: ???

##### MUGS #####
mugs_vitb16_ep400_extractB:
  ssl:
    objective: mugs
    ssl_mode: hierarchical  # both global and low level features (contrast patches)
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048 # 2048 for proj and 4096 for pred
    projection_hid_depth: 6 # 3 for proj and 3 for pred
    architecture_exact: vitb16 cls+avg
    architecture: vitb
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 85798656
    projection_nparameters: 73e6
  representation:
    z_dim: 1536
    z_layer: cls+avg
  optimization:
    epochs: 400
    batch_size: 1008
    optimizer: adamw
    learning_rate:  0.0008
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarize
      - autoaugment # for student
      - erasing
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 78.0
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/sail-sg/mugs
    notes: evaluation of the official model is unclear and eval code is not available but I think it's like DINO.
    month: 3
    year: 2022
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 24
    pu_type: A100
    time_hours: 64

mugs_vits16_ep800_extractS:
  ssl:
    objective: mugs
    ssl_mode: hierarchical  # both global and low level features (contrast patches)
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048 # 2048 for proj and 4096 for pred
    projection_hid_depth: 6 # 3 for proj and 3 for pred
    architecture_exact: vits16 4xcls
    architecture: vits
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 21665664
    projection_nparameters: 73e6
  representation:
    z_dim: 1536
    z_layer: 4xcls
  optimization:
    epochs: 800
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.0008
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarize
      - autoaugment # for student
      - erasing
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 75.6
    top1acc_in1k-1%_official: 66.9
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/sail-sg/mugs
    notes: evaluation of the official model is unclear and eval code is not available but I think it's like DINO.
    month: 3
    year: 2022
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 16
    pu_type: A100
    time_hours: 81

mugs_vitl16_ep250:
  ssl:
    objective: mugs
    ssl_mode: hierarchical  # both global and low level features (contrast patches)
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048 # 2048 for proj and 4096 for pred
    projection_hid_depth: 6 # 3 for proj and 3 for pred
    architecture_exact: vitl16 cls
    architecture: vitl
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 303301632
    projection_nparameters: 73e6
  representation:
    z_dim: 1024
    z_layer: cls
  optimization:
    epochs: 400
    batch_size: 640
    optimizer: adamw
    learning_rate:  0.0015
    weight_decay: 0.025
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarize
      - autoaugment # for student
      - erasing
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/sail-sg/mugs
    notes: evaluation of the official model is unclear and eval code is not available but I think it's like DINO.
    month: 3
    year: 2022
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 40
    pu_type: A100
    time_hours: 120

mugs_vitb16_ep400:
  ssl:
    objective: mugs
    ssl_mode: hierarchical  # both global and low level features (contrast patches)
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048 # 2048 for proj and 4096 for pred
    projection_hid_depth: 6 # 3 for proj and 3 for pred
    architecture_exact: vitb16 cls
    architecture: vitb
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 85798656
    projection_nparameters: 73e6
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 400
    batch_size: 1008
    optimizer: adamw
    learning_rate:  0.0008
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarize
      - autoaugment # for student
      - erasing
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/sail-sg/mugs
    notes: evaluation of the official model is unclear and eval code is not available but I think it's like DINO.
    month: 3
    year: 2022
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: True
  compute:
    n_pus: 24
    pu_type: A100
    time_hours: 64

mugs_vits16_ep800:
  ssl:
    objective: mugs
    ssl_mode: hierarchical  # both global and low level features (contrast patches)
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048 # 2048 for proj and 4096 for pred
    projection_hid_depth: 6 # 3 for proj and 3 for pred
    architecture_exact: vits16 cls
    architecture: vits
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 21665664
    projection_nparameters: 73e6
  representation:
    z_dim: 384
    z_layer: cls
  optimization:
    epochs: 800
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.0008
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarize
      - autoaugment # for student
      - erasing
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/sail-sg/mugs
    notes: evaluation of the official model is unclear and eval code is not available but I think it's like DINO.
    month: 3
    year: 2022
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: true
  compute:
    n_pus: 16
    pu_type: A100
    time_hours: 81

mugs_vits16_ep300:
  ssl:
    objective: mugs
    ssl_mode: hierarchical  # both global and low level features (contrast patches)
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048 # 2048 for proj and 4096 for pred
    projection_hid_depth: 6 # 3 for proj and 3 for pred
    architecture_exact: vits16 cls
    architecture: vits
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 21665664
    projection_nparameters: 73e6
  representation:
    z_dim: 384
    z_layer: cls
  optimization:
    epochs: 300
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.0008
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarize
      - autoaugment # for student
      - erasing
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/sail-sg/mugs
    notes: evaluation of the official model is unclear and eval code is not available but I think it's like DINO.
    month: 3
    year: 2022
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: true
  compute:
    n_pus: 16
    pu_type: A100
    time_hours: 108

mugs_vits16_ep100:
  ssl:
    objective: mugs
    ssl_mode: hierarchical  # both global and low level features (contrast patches)
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048 # 2048 for proj and 4096 for pred
    projection_hid_depth: 6 # 3 for proj and 3 for pred
    architecture_exact: vits16 cls
    architecture: vits
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 21665664
    projection_nparameters: 73e6
  representation:
    z_dim: 384
    z_layer: cls
  optimization:
    epochs: 100
    batch_size: 512
    optimizer: adamw
    learning_rate:  0.0008
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarize
      - autoaugment # for student
      - erasing
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/sail-sg/mugs
    notes: evaluation of the official model is unclear and eval code is not available but I think it's like DINO.
    month: 3
    year: 2022
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: true
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: 27

mugs_vitl16_ep250_extractB:
  ssl:
    objective: mugs
    ssl_mode: hierarchical  # both global and low level features (contrast patches)
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048 # 2048 for proj and 4096 for pred
    projection_hid_depth: 6 # 3 for proj and 3 for pred
    architecture_exact: vitl16 cls+avg
    architecture: vitl
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 303301632
    projection_nparameters: 73e6
  representation:
    z_dim: 2048
    z_layer: cls+avg
  optimization:
    epochs: 400
    batch_size: 640
    optimizer: adamw
    learning_rate:  0.0015
    weight_decay: 0.025
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarize
      - autoaugment # for student
      - erasing
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/sail-sg/mugs
    notes: evaluation of the official model is unclear and eval code is not available but I think it's like DINO.
    month: 3
    year: 2022
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: true
  compute:
    n_pus: 40
    pu_type: A100
    time_hours: 120


mugs_vitl16_ep250_extractS:
  ssl:
    objective: mugs
    ssl_mode: hierarchical  # both global and low level features (contrast patches)
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048 # 2048 for proj and 4096 for pred
    projection_hid_depth: 6 # 3 for proj and 3 for pred
    architecture_exact: vitl16 4xcls
    architecture: vitl
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 303301632
    projection_nparameters: 73e6
  representation:
    z_dim: 4096
    z_layer: 4xcls
  optimization:
    epochs: 400
    batch_size: 640
    optimizer: adamw
    learning_rate:  0.0015
    weight_decay: 0.025
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarize
      - autoaugment # for student
      - erasing
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/sail-sg/mugs
    notes: evaluation of the official model is unclear and eval code is not available but I think it's like DINO.
    month: 3
    year: 2022
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: true
  compute:
    n_pus: 40
    pu_type: A100
    time_hours: 120


mugs_vitb16_ep400_extractS:
  ssl:
    objective: mugs
    ssl_mode: hierarchical  # both global and low level features (contrast patches)
    version: 1
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048 # 2048 for proj and 4096 for pred
    projection_hid_depth: 6 # 3 for proj and 3 for pred
    architecture_exact: vitb16 4xcls
    architecture: vitb
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 85798656
    projection_nparameters: 73e6
  representation:
    z_dim: 3072
    z_layer: 4xcls
  optimization:
    epochs: 400
    batch_size: 1008
    optimizer: adamw
    learning_rate:  0.0008
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarize
      - autoaugment # for student
      - erasing
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/sail-sg/mugs
    notes: evaluation of the official model is unclear and eval code is not available but I think it's like DINO.
    month: 3
    year: 2022
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: true
  compute:
    n_pus: 24
    pu_type: A100
    time_hours: 64



##### IBOT #####
ibot_vitS16_extractS:
  ssl:
    objective: ibot
    ssl_mode: clustering #actually it's clustering of patches => not really clustering
    version: 1
    n_negatives: 12288
    n_classes: 8192
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vits16 4xcls
    architecture: vits
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 85798656
    projection_nparameters: 13e6
  representation:
    z_dim: 1536
    z_layer: 4xcls
  optimization:
    epochs: 800
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.0005
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarize
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 77.9
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/bytedance/ibot
    notes: prediction shape is block
    month: 11
    year: 2021
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 16
    pu_type: V100
    time_hours: 193.4

ibot_vitB16_extractB:
  ssl:
    objective: ibot
    ssl_mode: clustering #actually it's clustering of patches => not really clustering
    version: 1
    n_negatives: 12288
    n_classes: 8192
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitb16 cls+avg
    architecture: vitb
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 85798656
    projection_nparameters: 13e6
  representation:
    z_dim: 1536
    z_layer: cls+avg
  optimization:
    epochs: 400
    batch_size: 1032
    optimizer: adamw
    learning_rate:  0.00075
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarize
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 79.5
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/bytedance/ibot
    notes: prediction shape is block
    month: 11
    year: 2021
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 24
    pu_type: V100
    time_hours: ???

ibot_vitL16:
  ssl:
    objective: ibot
    ssl_mode: clustering #actually it's clustering of patches => not really clustering
    version: 1
    n_negatives: 12288
    n_classes: 8192
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitl16 cls
    architecture: vitl
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 303301632
    projection_nparameters: 13e6
  representation:
    z_dim: 1024
    z_layer: cls
  optimization:
    epochs: 250
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.0005
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarize
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/bytedance/ibot
    notes: prediction shape is block
    month: 11
    year: 2021
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 64
    pu_type: V100
    time_hours: ???


ibot_vitL16_extractB:
  ssl:
    objective: ibot
    ssl_mode: clustering #actually it's clustering of patches => not really clustering
    version: 1
    n_negatives: 12288
    n_classes: 8192
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitl16 cls+avg
    architecture: vitl
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 303301632
    projection_nparameters: 13e6
  representation:
    z_dim: 2048
    z_layer: cls+avg
  optimization:
    epochs: 250
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.0005
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarize
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/bytedance/ibot
    notes: prediction shape is block
    month: 11
    year: 2021
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: true
  compute:
    n_pus: 64
    pu_type: V100
    time_hours: ???

ibot_vitL16_extractS:
  ssl:
    objective: ibot
    ssl_mode: clustering #actually it's clustering of patches => not really clustering
    version: 1
    n_negatives: 12288
    n_classes: 8192
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitl16 4xcls
    architecture: vitl
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 303301632
    projection_nparameters: 13e6
  representation:
    z_dim: 4096
    z_layer: 4xcls
  optimization:
    epochs: 250
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.0005
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarize
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/bytedance/ibot
    notes: prediction shape is block
    month: 11
    year: 2021
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: true
  compute:
    n_pus: 64
    pu_type: V100
    time_hours: ???



ibot_vitS16:
  ssl:
    objective: ibot
    ssl_mode: clustering #actually it's clustering of patches => not really clustering
    version: 1
    n_negatives: 12288
    n_classes: 8192
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vits16 cls
    architecture: vits
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 21665664
    projection_nparameters: 13e6
  representation:
    z_dim: 384
    z_layer: cls
  optimization:
    epochs: 800
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.0005
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarize
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/bytedance/ibot
    notes: prediction shape is block
    month: 11
    year: 2021
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: true
  compute:
    n_pus: 16
    pu_type: V100
    time_hours: 193.4

ibot_vitB16:
  ssl:
    objective: ibot
    ssl_mode: clustering #actually it's clustering of patches => not really clustering
    version: 1
    n_negatives: 12288
    n_classes: 8192
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitb16 cls
    architecture: vitb
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 85798656
    projection_nparameters: 13e6
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 400
    batch_size: 1032
    optimizer: adamw
    learning_rate:  0.00075
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - masking
      - solarize
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/bytedance/ibot
    notes: prediction shape is block
    month: 11
    year: 2021
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: true
  compute:
    n_pus: 24
    pu_type: V100
    time_hours: ???

##### DINO #####
dino_vitB16_extractS:
  ssl:
    objective: dino
    ssl_mode: clustering
    version: 1
    n_negatives: 12288
    n_classes: 65536
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitb16 4xcls
    architecture: vitb
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 85798656
    projection_nparameters: 26e6
  representation:
    z_dim: 3072
    z_layer: 4xcls
  optimization:
    epochs: 400
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.00075
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - solarize
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/dino
    notes: null
    month: 4
    year: 2021
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: true
  compute:
    n_pus: 32
    pu_type: V100
    time_hours: ???

dino_vitS16_extractB:
  ssl:
    objective: dino
    ssl_mode: clustering
    version: 1
    n_negatives: 12288
    n_classes: 65536
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vits16 cls+avg
    architecture: vits
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 21665664
    projection_nparameters: 26e6
  representation:
    z_dim: 768
    z_layer: cls+avg
  optimization:
    epochs: 800
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.0005
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - solarize
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/dino
    notes: null
    month: 4
    year: 2021
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: true
  compute:
    n_pus: 16
    pu_type: V100
    time_hours: 194

dino_vitB8:
  ssl:
    objective: dino
    ssl_mode: clustering
    version: 1
    n_negatives: 12288
    n_classes: 65536
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitb8 cls+avg
    architecture: vitb
    family: vit
    patch_size: 8
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 85807872
    projection_nparameters: 26e6
  representation:
    z_dim: 1536
    z_layer: cls+avg
  optimization:
    epochs: 300
    batch_size: 1056
    optimizer: adamw
    learning_rate:  0.0005
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - solarize
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 80.1
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/dino
    notes: null
    month: 4
    year: 2021
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 176
    pu_type: V100
    time_hours: ???

dino_vitB16:
  ssl:
    objective: dino
    ssl_mode: clustering
    version: 1
    n_negatives: 12288
    n_classes: 65536
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitb16 cls+avg
    architecture: vitb
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 85798656
    projection_nparameters: 26e6
  representation:
    z_dim: 1536
    z_layer: cls+avg
  optimization:
    epochs: 400
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.00075
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - solarize
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 78.2
    top1acc_in1k-1%_official: 64.5
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/dino
    notes: null
    month: 4
    year: 2021
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 32
    pu_type: V100
    time_hours: ???

dino_vitS16:
  ssl:
    objective: dino
    ssl_mode: clustering
    version: 1
    n_negatives: 12288
    n_classes: 65536
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vits16 4xcls
    architecture: vits
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 21665664
    projection_nparameters: 26e6
  representation:
    z_dim: 1536
    z_layer: 4xcls
  optimization:
    epochs: 800
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.0005
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - solarize
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 77.0
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/dino
    notes: null
    month: 4
    year: 2021
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 16
    pu_type: V100
    time_hours: 194

dino_vitB8_last:
  ssl:
    objective: dino
    ssl_mode: clustering
    version: 1
    n_negatives: 12672
    n_classes: 65536
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitb8 cls
    architecture: vitb
    family: vit
    patch_size: 8
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 85807872
    projection_nparameters: 26e6
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 300
    batch_size: 1056
    optimizer: adamw
    learning_rate:  0.0005
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - solarize
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/dino
    notes: null
    month: 4
    year: 2021
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: true
  compute:
    n_pus: 176
    pu_type: V100
    time_hours: ???

dino_vitB16_last:
  ssl:
    objective: dino
    ssl_mode: clustering
    version: 1
    n_negatives: 12288
    n_classes: 65536
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vitb16 cls
    architecture: vitb
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 85798656
    projection_nparameters: 26e6
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 400
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.00075
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - solarize
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/dino
    notes: null
    month: 4
    year: 2021
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: true
  compute:
    n_pus: 32
    pu_type: V100
    time_hours: ???

dino_vitS8_last:
  ssl:
    objective: dino
    ssl_mode: clustering
    version: 1
    n_negatives: 12288
    n_classes: 65536
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vits8 cls
    architecture: vits
    family: vit
    patch_size: 8
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 21670272
    projection_nparameters: 26e6
  representation:
    z_dim: 384
    z_layer: cls
  optimization:
    epochs: 800
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.0005
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - solarize
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/dino
    notes: null
    month: 4
    year: 2021
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: true
  compute:
    n_pus: 64
    pu_type: V100
    time_hours: ???

dino_vitS16_last:
  ssl:
    objective: dino
    ssl_mode: clustering
    version: 1
    n_negatives: 12288
    n_classes: 65536
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: vits16 cls
    architecture: vits
    family: vit
    patch_size: 16
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 21665664
    projection_nparameters: 26e6
  representation:
    z_dim: 384
    z_layer: cls
  optimization:
    epochs: 800
    batch_size: 1024
    optimizer: adamw
    learning_rate:  0.0005
    weight_decay: 0.04
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+10x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - solarize
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/dino
    notes: null
    month: 4
    year: 2021
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: true
  compute:
    n_pus: 16
    pu_type: V100
    time_hours: 194

dino_rn50:
  ssl:
    objective: dino
    ssl_mode: clustering
    version: 1
    n_negatives: 32640
    n_classes: 60000
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 2048
    projection_hid_depth: 3
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 24e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 800
    batch_size: 4080
    optimizer: lars
    learning_rate:  0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+6x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - gray
      - blur
      - solarize
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 75.3
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/dino
    notes: null
    month: 4
    year: 2021
    license: Apache License 2.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 80
    pu_type: V100
    time_hours: ???


##### CLIP #####
clip_vitL14_px336:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 32768
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 768
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vitl14 cls
    architecture: vitl
    family: vit
    patch_size: 14
    projection1_arch: linear
    projection2_arch: linear
    projection_same: false
    n_parameters: 303507456
    projection_nparameters: 1e6
  representation:
    z_dim: 1024
    z_layer: cls
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 4e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: null
    img_size: 336
    views: 2x336
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 85.4
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/openai/CLIP
    notes: null
    month: 1
    year: 2021
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 256
    pu_type: V100
    time_hours: 288

clip_vitL14_px336_extractPred:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 32768
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 768
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vitl14 proj
    architecture: vitl
    family: vit
    patch_size: 14
    projection1_arch: linear
    projection2_arch: linear
    projection_same: false
    n_parameters: 303507456
    projection_nparameters: 1e6
  representation:
    z_dim: 768
    z_layer: proj
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 4e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: null
    img_size: 336
    views: 2x336
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/openai/CLIP
    notes: null
    month: 1
    year: 2021
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: true
  compute:
    n_pus: 256
    pu_type: V100
    time_hours: 288

clip_vitL14_px336_extractPredCls:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 32768
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 768
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vitl14 proj+cls
    architecture: vitl
    family: vit
    patch_size: 14
    projection1_arch: linear
    projection2_arch: linear
    projection_same: false
    n_parameters: 303507456
    projection_nparameters: 1e6
  representation:
    z_dim: 1792
    z_layer: proj+cls
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 4e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: null
    img_size: 336
    views: 2x336
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/openai/CLIP
    notes: null
    month: 1
    year: 2021
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: true
  compute:
    n_pus: 256
    pu_type: V100
    time_hours: 288

clip_vitL14_px336_extractS:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 32768
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 768
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vitl14 4xcls
    architecture: vitl
    family: vit
    patch_size: 14
    projection1_arch: linear
    projection2_arch: linear
    projection_same: false
    n_parameters: 303507456
    projection_nparameters: 1e6
  representation:
    z_dim: 4096
    z_layer: 4xcls
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 4e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: null
    img_size: 336
    views: 2x336
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/openai/CLIP
    notes: null
    month: 1
    year: 2021
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: true
  compute:
    n_pus: 256
    pu_type: V100
    time_hours: 288

clip_vitL14_px336_extractB:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 32768
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 768
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vitl14 cls+avg
    architecture: vitl
    family: vit
    patch_size: 14
    projection1_arch: linear
    projection2_arch: linear
    projection_same: false
    n_parameters: 303507456
    projection_nparameters: 1e6
  representation:
    z_dim: 2048
    z_layer: cls+avg
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 4e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: null
    img_size: 336
    views: 2x336
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/openai/CLIP
    notes: null
    month: 1
    year: 2021
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: true
  compute:
    n_pus: 256
    pu_type: V100
    time_hours: 288

clip_vitL14:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 32768
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 768
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vitl14 cls
    architecture: vitl
    family: vit
    patch_size: 14
    projection1_arch: linear
    projection2_arch: linear
    projection_same: false
    n_parameters: 303179776
    projection_nparameters: 1e6
  representation:
    z_dim: 1024
    z_layer: cls
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 4e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 83.9
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/openai/CLIP
    notes: null
    month: 1
    year: 2021
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: ???
    pu_type: V100
    time_hours: ???

clip_vitB32:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 32768
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vitb32 cls
    architecture: vitb
    family: vit
    patch_size: 32
    projection1_arch: linear
    projection2_arch: linear
    projection_same: false
    n_parameters: 85799424
    projection_nparameters: 0.4e6
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 5e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 76.1
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/openai/CLIP
    notes: null
    month: 1
    year: 2021
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: ???
    pu_type: V100
    time_hours: ???

clip_vitB16:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 32768
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vitb16 cls
    architecture: vitb
    family: vit
    patch_size: 16
    projection1_arch: linear
    projection2_arch: linear
    projection_same: false
    n_parameters: 85799424
    projection_nparameters: 0.4e6
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 5e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 80.2
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/openai/CLIP
    notes: null
    month: 1
    year: 2021
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: ???
    pu_type: V100
    time_hours: ???

clip_rn101:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 32768
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact:  clipresnet101
    architecture: resnet101
    family: resnet
    patch_size: null
    projection1_arch: linear
    projection2_arch: linear
    projection_same: false
    n_parameters: 59407200
    projection_nparameters: 2e6
  representation:
    z_dim: 2048
    z_layer: attnpool
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 3.6e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 75.7
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/openai/CLIP
    notes: unclear for resnet if CLIP uses features before or after the projection head. We use before
    month: 1
    year: 2021
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: ???
    pu_type: V100
    time_hours: ???

clip_rn50x64:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 32768
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 1024
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact:  clipresnet50w64
    architecture: resnet50w64
    family: resnet
    patch_size: null
    projection1_arch: linear
    projection2_arch: linear
    projection_same: false
    n_parameters: 432966336
    projection_nparameters: 2e6
  representation:
    z_dim: 4096
    z_layer: attnpool
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 3.6e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: null
    img_size: 448
    views: 2x448
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 83.6
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/openai/CLIP
    notes: unclear for resnet if CLIP uses features before or after the projection head. We use before
    month: 1
    year: 2021
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 592
    pu_type: V100
    time_hours: 432

clip_rn50x16:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 32768
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 768
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact:  clipresnet50w16
    architecture: resnet50w16
    family: resnet
    patch_size: null
    projection1_arch: linear
    projection2_arch: linear
    projection_same: false
    n_parameters: 174409104
    projection_nparameters: 2e6
  representation:
    z_dim: 3072
    z_layer: attnpool
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 4e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: null
    img_size: 384
    views: 2x384
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 81.5
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/openai/CLIP
    notes: unclear for resnet if CLIP uses features before or after the projection head. We use before
    month: 1
    year: 2021
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: ???
    pu_type: V100
    time_hours: ???

clip_rn50x4:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 32768
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 1024
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact:  clipresnet50w4
    architecture: resnet50w4
    family: resnet
    patch_size: null
    projection1_arch: linear
    projection2_arch: linear
    projection_same: false
    n_parameters: 92054200
    projection_nparameters: 2e6
  representation:
    z_dim: 640
    z_layer: attnpool
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 5e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: null
    img_size: 288
    views: 2x288
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 78.2
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/openai/CLIP
    notes: unclear for resnet if CLIP uses features before or after the projection head. We use before
    month: 1
    year: 2021
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: ???
    pu_type: V100
    time_hours: ???

clip_rn50:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 32768
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 1024
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: clipresnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: linear
    projection2_arch: linear
    projection_same: false
    n_parameters: 38317920
    projection_nparameters: 2e6
  representation:
    z_dim: 2048
    z_layer: attnpool
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 5e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 73.3
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/openai/CLIP
    notes: unclear for resnet if CLIP uses features before or after the projection head. We use before
    month: 1
    year: 2021
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: ???
    pu_type: V100
    time_hours: ???


##### OPENCLIP #####

openclip_vitB32:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 79104
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vitb32 cls
    architecture: vitb
    family: vit
    patch_size: 32
    projection1_arch: linear
    projection2_arch: linear
    projection_same: false
    n_parameters: 87849216
    projection_nparameters: 3e5
  representation:
    z_dim: 768
    z_layer: cls
  optimization:
    epochs: 32
    batch_size: 79104
    optimizer: adamw # larc
    learning_rate: 1e-3
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: LAION-2B
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 76.9
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/mlfoundations/open_clip
    notes: null
    month: 12
    year: 2022
    license: MIT
    is_official: true
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 824
    pu_type: A100
    time_hours: 51

openclip_vitL14:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 86016
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 768
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vitl14 cls
    architecture: vitl
    family: vit
    patch_size: 14
    projection1_arch: linear
    projection2_arch: linear
    projection_same: false
    n_parameters: 303966208
    projection_nparameters: 8e5
  representation:
    z_dim: 1024
    z_layer: cls
  optimization:
    epochs: 32
    batch_size: 86016
    optimizer: adamw # larc
    learning_rate: 1e-3
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: LAION-2B
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 83.5
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/mlfoundations/open_clip
    notes: null
    month: 12
    year: 2022
    license: MIT
    is_official: true
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 1024
    pu_type: A100
    time_hours: 122

openclip_vitH14:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 79104
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 1024
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vith14 cls
    architecture: vith
    family: vit
    patch_size: 14
    projection1_arch: linear
    projection2_arch: linear
    projection_same: false
    n_parameters: 632076800
    projection_nparameters: 1e6
  representation:
    z_dim: 1280
    z_layer: cls
  optimization:
    epochs: 32
    batch_size: 79104
    optimizer: adamw # larc
    learning_rate: 5e-4
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: LAION-2B
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 84.9
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/mlfoundations/open_clip
    notes: null
    month: 12
    year: 2022
    license: MIT
    is_official: true
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 824
    pu_type: A100
    time_hours: 279

openclip_vitH14_extractPred:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 79104
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 1024
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vith14 proj
    architecture: vith
    family: vit
    patch_size: 14
    projection1_arch: linear
    projection2_arch: linear
    projection_same: false
    n_parameters: 632076800
    projection_nparameters: 1e6
  representation:
    z_dim: 1024
    z_layer: proj
  optimization:
    epochs: 32
    batch_size: 79104
    optimizer: adamw # larc
    learning_rate: 5e-4
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: LAION-2B
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/mlfoundations/open_clip
    notes: null
    month: 12
    year: 2022
    license: MIT
    is_official: true
    is_industry: false
    is_our_extract: true
  compute:
    n_pus: 824
    pu_type: A100
    time_hours: 279

openclip_vitH14_extractS:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 79104
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 1024
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vith14 4xcls
    architecture: vith
    family: vit
    patch_size: 14
    projection1_arch: linear
    projection2_arch: linear
    projection_same: false
    n_parameters: 632076800
    projection_nparameters: 1e6
  representation:
    z_dim: 5120
    z_layer: 4xcls
  optimization:
    epochs: 32
    batch_size: 79104
    optimizer: adamw # larc
    learning_rate: 5e-4
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: LAION-2B
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/mlfoundations/open_clip
    notes: null
    month: 12
    year: 2022
    license: MIT
    is_official: true
    is_industry: false
    is_our_extract: true
  compute:
    n_pus: 824
    pu_type: A100
    time_hours: 279

openclip_vitH14_extractB:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 79104
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 1024
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vith14 cls+avg
    architecture: vith
    family: vit
    patch_size: 14
    projection1_arch: linear
    projection2_arch: linear
    projection_same: false
    n_parameters: 632076800
    projection_nparameters: 1e6
  representation:
    z_dim: 2560
    z_layer: cls+avg
  optimization:
    epochs: 32
    batch_size: 79104
    optimizer: adamw # larc
    learning_rate: 5e-4
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: LAION-2B
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/mlfoundations/open_clip
    notes: null
    month: 12
    year: 2022
    license: MIT
    is_official: true
    is_industry: false
    is_our_extract: true
  compute:
    n_pus: 824
    pu_type: A100
    time_hours: 279

openclip_vitG14:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 64000
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 1024
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vitg14 cls
    architecture: vitg
    family: vit
    patch_size: 14
    projection1_arch: linear
    projection2_arch: linear
    projection_same: false
    n_parameters: 1012645632
    projection_nparameters: 1e6
  representation:
    z_dim: 1408
    z_layer: cls
  optimization:
    epochs: 32
    batch_size: 64000
    optimizer: adamw # larc
    learning_rate: 5e-4
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: LAION-2B
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 84.1
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/mlfoundations/open_clip
    notes: null
    month: 12
    year: 2022
    license: MIT
    is_official: true
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 800
    pu_type: A100
    time_hours: 137

openclip_vitG14_extractPred:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 64000
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 1024
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vitg14 proj
    architecture: vitg
    family: vit
    patch_size: 14
    projection1_arch: linear
    projection2_arch: linear
    projection_same: false
    n_parameters: 1012645632
    projection_nparameters: 1e6
  representation:
    z_dim: 1024
    z_layer: proj
  optimization:
    epochs: 32
    batch_size: 64000
    optimizer: adamw # larc
    learning_rate: 5e-4
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: LAION-2B
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/mlfoundations/open_clip
    notes: null
    month: 12
    year: 2022
    license: MIT
    is_official: true
    is_industry: false
    is_our_extract: true
  compute:
    n_pus: 800
    pu_type: A100
    time_hours: 137

openclip_vitG14_extractS:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 64000
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 1024
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vitg14 4xcls
    architecture: vitg
    family: vit
    patch_size: 14
    projection1_arch: linear
    projection2_arch: linear
    projection_same: false
    n_parameters: 1012645632
    projection_nparameters: 1e6
  representation:
    z_dim: 5632
    z_layer: 4xcls
  optimization:
    epochs: 32
    batch_size: 64000
    optimizer: adamw # larc
    learning_rate: 5e-4
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: LAION-2B
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/mlfoundations/open_clip
    notes: null
    month: 12
    year: 2022
    license: MIT
    is_official: true
    is_industry: false
    is_our_extract: true
  compute:
    n_pus: 800
    pu_type: A100
    time_hours: 137


openclip_vitG14_extractB:
  ssl:
    objective: clip
    ssl_mode: contrastive
    version: 1
    n_negatives: 64000
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 1024
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vitg14 cls+avg
    architecture: vitg
    family: vit
    patch_size: 14
    projection1_arch: linear
    projection2_arch: linear
    projection_same: false
    n_parameters: 1012645632
    projection_nparameters: 1e6
  representation:
    z_dim: 2816
    z_layer: cls+avg
  optimization:
    epochs: 32
    batch_size: 64000
    optimizer: adamw # larc
    learning_rate: 5e-4
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: LAION-2B
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/mlfoundations/open_clip
    notes: null
    month: 12
    year: 2022
    license: MIT
    is_official: true
    is_industry: false
    is_our_extract: true
  compute:
    n_pus: 800
    pu_type: A100
    time_hours: 137

##### Lossyless #####
lossyless_vitb32_b01:
  ssl:
    objective: clip+eb
    ssl_mode: contrastive
    version: 1
    n_negatives: 32768
    n_classes: null
    is_stopgrad: false
    is_ema: false
    other: beta 0.1
  model:
    pred_dim: 512
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vitb32 pred
    architecture: vitb
    family: vit
    patch_size: 32
    projection1_arch: identity
    projection2_arch: identity
    projection_same: true
    n_parameters: 85799424
    projection_nparameters: 0
  representation:
    z_dim: 512
    z_layer: pred
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 5e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: mscoco
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 76.0
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/YannDubs/lossyless
    notes: the only difference compared to clip is an invertible linear layer one top of representations that was trained on mscoco to compress the representations. b denotes `beta`, larger means more compression.
    month: 6
    year: 2021
    license: MIT
    is_official: true
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: ???
    pu_type: V100
    time_hours: ???

lossyless_vitb32_b005:
  ssl:
    objective: clip+eb
    ssl_mode: contrastive
    version: 1
    n_negatives: 32768
    n_classes: null
    is_stopgrad: false
    is_ema: false
    other: beta 0.05
  model:
    pred_dim: 512
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vitb32 pred
    architecture: vitb
    family: vit
    patch_size: 32
    projection1_arch: identity
    projection2_arch: identity
    projection_same: true
    n_parameters: 85799424
    projection_nparameters: 0
  representation:
    z_dim: 512
    z_layer: pred
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 5e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: mscoco
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 76.3
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/YannDubs/lossyless
    notes: the only difference compared to clip is an invertible linear layer one top of representations that was trained on mscoco to compress the representations. b denotes `beta`, larger means more compression.
    month: 6
    year: 2021
    license: MIT
    is_official: true
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: ???
    pu_type: V100
    time_hours: ???

lossyless_vitb32_b001:
  ssl:
    objective: clip+eb
    ssl_mode: contrastive
    version: 1
    n_negatives: 32768
    n_classes: null
    is_stopgrad: false
    is_ema: false
    other: beta 0.01
  model:
    pred_dim: 512
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: vitb32 pred
    architecture: vitb
    family: vit
    patch_size: 32
    projection1_arch: identity
    projection2_arch: identity
    projection_same: true
    n_parameters: 85799424
    projection_nparameters: 0
  representation:
    z_dim: 512
    z_layer: pred
  optimization:
    epochs: 32
    batch_size: 32768
    optimizer: adamw # larc
    learning_rate: 5e-4 # not sure if this is correct, it's unclear if that the actual one or the one before linear scaling due to large batch size
    weight_decay: 0.2
    scheduler: cosine
  data:
    pretraining_data: CLIP
    finetuning_data: mscoco
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 76.6
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/YannDubs/lossyless
    notes: the only difference compared to clip is an invertible linear layer one top of representations that was trained on mscoco to compress the representations. b denotes `beta`, larger means more compression.
    month: 6
    year: 2021
    license: MIT
    is_official: true
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: ???
    pu_type: V100
    time_hours: ???

##### Risk Dec #####
speccl_resnet50_bs384_ep100:
  ssl:
    objective: speccl
    ssl_mode: contrastive
    version: 1
    n_negatives: 5120
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 8192
    projection_hid_width: 8192
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 31128640
    projection_nparameters: 151e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 384
    optimizer: sgd # larc
    learning_rate: 0.033
    weight_decay: 1e-4
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 66.97
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes: null
    month: 6
    year: 2021
    license: MIT
    is_official: true
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: GTX 1080
    time_hours: ???

simclr_resnet50_d8192_e100_m2:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 5120
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50d8192
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 31128640
    projection_nparameters: 17e6
  representation:
    z_dim: 8192
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes: null
    month: 2
    year: 2020
    license: MIT
    is_official: false
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

simclr_resnet50_dNone_e100_m2_headTNoneSNone:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 5120
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 2048
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: identity
    projection2_arch: identity
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 0
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes: no projection head
    month: 2
    year: 2020
    license: MIT
    is_official: false
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

simclr_resnet50_dNone_e100_m2_headTMlpSMlp:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 5120
    n_classes: null
    is_stopgrad: false
    is_ema: false
    other: headTMlpSMlp
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: false
    n_parameters: 23508032
    projection_nparameters: 9e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes: both teacher and student have same mlp projection heads but weights are not shared
    month: 2
    year: 2020
    license: MIT
    is_official: false
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

simclr_resnet50_dNone_e100_m2_headTMlpSLin:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 5120
    n_classes: null
    is_stopgrad: false
    is_ema: false
    other: headTMlpSLin
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: linear
    projection_same: false
    n_parameters: 23508032
    projection_nparameters: 9e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes: student has linear head
    month: 2
    year: 2020
    license: MIT
    is_official: false
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

simclr_resnet50_dNone_e100_m2_headTLinSLin:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 5120
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: linear
    projection2_arch: linear
    projection_same: false
    n_parameters: 23508032
    projection_nparameters: 0.3e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes: both teacher and student have linear projection heads
    month: 2
    year: 2020
    license: MIT
    is_official: false
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

simclr_resnet50_dNone_e100_m2_data030:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 5120
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 9e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 300
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: 30% ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes:  trains on 30% of imagenet but multiplies by 3 number of epochs to keep same steps.
    month: 2
    year: 2020
    license: MIT
    is_official: false
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

simclr_resnet50_dNone_e100_m2_data010:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 5120
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 9e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: 10% ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes:  trains on 10% of imagenet but multiplies by 10 number of epochs to keep same steps.
    month: 2
    year: 2020
    license: MIT
    is_official: false
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

simclr_resnet50_dNone_e100_m2:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 5120
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 9e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes:  null
    month: 6
    year: 2020
    license: MIT
    is_official: false
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

dissl_resnet50_d4096_e100_m2:
  ssl:
    objective: dissl
    ssl_mode: clustering
    version: 1
    n_negatives: 5120
    n_classes: 16384
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50d4096
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: linear
    projection_same: false
    n_parameters: 29023296
    projection_nparameters: 16e6
  representation:
    z_dim: 4096
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes:  null
    month: 6
    year: 2022
    license: MIT
    is_official: false
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

dissl_resnet50_dNone_e100_m2_headTMlpSMlp:
  ssl:
    objective: dissl
    ssl_mode: clustering
    version: 1
    n_negatives: 5120
    n_classes: 16384
    is_stopgrad: false
    is_ema: false
    other: headTMlpSMlp
  model:
    pred_dim: 512
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: false
    n_parameters: 23508032
    projection_nparameters: 17e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes:  uses mlp projection head for teacher and student
    month: 6
    year: 2022
    license: MIT
    is_official: false
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

dissl_resnet50_dNone_e100_m2_headTLinSLin:
  ssl:
    objective: dissl
    ssl_mode: clustering
    version: 1
    n_negatives: 5120
    n_classes: 16384
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: linear
    projection2_arch: linear
    projection_same: false
    n_parameters: 23508032
    projection_nparameters: 9e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes:  uses linear projection head for teacher and student
    month: 6
    year: 2022
    license: MIT
    is_official: false
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

dissl_resnet50_dNone_e100_m2_augSmall:
  ssl:
    objective: dissl
    ssl_mode: clustering
    version: 1
    n_negatives: 5120
    n_classes: 16384
    is_stopgrad: false
    is_ema: false
    other: augSmall
  model:
    pred_dim: 512
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: linear
    projection_same: false
    n_parameters: 23508032
    projection_nparameters: 17e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes: decreases strength of crop and color distortion
    month: 6
    year: 2022
    license: MIT
    is_official: false
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

dissl_resnet50_dNone_e100_m2_augLarge:
  ssl:
    objective: dissl
    ssl_mode: clustering
    version: 1
    n_negatives: 5120
    n_classes: 16384
    is_stopgrad: false
    is_ema: false
    other: augLarge
  model:
    pred_dim: 512
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: linear
    projection_same: false
    n_parameters: 23508032
    projection_nparameters: 17e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/YannDubs/SSL-Risk-Decomposition
    notes: increases strength of crop and color distortion
    month: 6
    year: 2022
    license: MIT
    is_official: false
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

##### ISSL #####
dissl_resnet50_d8192_e800_m8:
  ssl:
    objective: dissl
    ssl_mode: clustering
    version: 1
    n_negatives: 20480
    n_classes: 16384
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50d8192
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: linear
    projection_same: false
    n_parameters: 31128640
    projection_nparameters: 17e6
  representation:
    z_dim: 8192
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+6x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 73.9
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/YannDubs/Invariant-Self-Supervised-Learning
    notes: null
    month: 6
    year: 2022
    license: MIT
    is_official: true
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

dissl_resnet50_d8192_e400_m6:
  ssl:
    objective: dissl
    ssl_mode: clustering
    version: 1
    n_negatives: 5120
    n_classes: 16384
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50d8192
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: linear
    projection_same: false
    n_parameters: 31128640
    projection_nparameters: 17e6
  representation:
    z_dim: 8192
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x160+4x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 74.0
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/YannDubs/Invariant-Self-Supervised-Learning
    notes: null
    month: 6
    year: 2022
    license: MIT
    is_official: true
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

dissl_resnet50_dNone_e400_m6:
  ssl:
    objective: dissl
    ssl_mode: clustering
    version: 1
    n_negatives: 5120
    n_classes: 16384
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: linear
    projection_same: false
    n_parameters: 23508032
    projection_nparameters: 17e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x160+4x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 73.0
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/YannDubs/Invariant-Self-Supervised-Learning
    notes: null
    month: 6
    year: 2022
    license: MIT
    is_official: true
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

dissl_resnet50_dNone_e400_m2:
  ssl:
    objective: dissl
    ssl_mode: clustering
    version: 1
    n_negatives: 5120
    n_classes: 16384
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: linear
    projection_same: false
    n_parameters: 31128640
    projection_nparameters: 17e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 71.1
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/YannDubs/Invariant-Self-Supervised-Learning
    notes: null
    month: 6
    year: 2022
    license: MIT
    is_official: true
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

dissl_resnet50_d8192_e100_m2:
  ssl:
    objective: dissl
    ssl_mode: clustering
    version: 1
    n_negatives: 5120
    n_classes: 16384
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50d8192
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: linear
    projection_same: false
    n_parameters: 31128640
    projection_nparameters: 17e6
  representation:
    z_dim: 8192
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 68.9
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/YannDubs/Invariant-Self-Supervised-Learning
    notes: null
    month: 6
    year: 2022
    license: MIT
    is_official: true
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

dissl_resnet50_dNone_e100_m2:
  ssl:
    objective: dissl
    ssl_mode: clustering
    version: 1
    n_negatives: 5120
    n_classes: 16384
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: linear
    projection_same: false
    n_parameters: 23508032
    projection_nparameters: 17e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 66.9
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/YannDubs/Invariant-Self-Supervised-Learning
    notes: null
    month: 6
    year: 2022
    license: MIT
    is_official: true
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: A100
    time_hours: ???

### SimSiam ###
simsiam_rn50_bs256_ep100:
  ssl:
    objective: simsiam
    ssl_mode: siamese
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: false
    n_parameters: 23508032
    projection_nparameters: 9e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 256
    optimizer: lars # larc
    learning_rate: 0.05
    weight_decay: 1e-4
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 68.3
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/simsiam
    notes: null
    month: 10
    year: 2020
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: ???
    pu_type: ???
    time_hours: ???

simsiam_rn50_bs512_ep100:
  ssl:
    objective: simsiam
    ssl_mode: siamese
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 512
    projection_hid_width: 2048
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: false
    n_parameters: 23508032
    projection_nparameters: 9e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 512
    optimizer: lars # larc
    learning_rate: 0.05
    weight_decay: 1e-4
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 68.1
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/simsiam
    notes: null
    month: 10
    year: 2020
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: ???
    pu_type: ???
    time_hours: ???

### SwAV ###
#selav2_rn50_ep400_2x160_4x96:
#  ssl:
#    objective: sela
#    ssl_mode: clustering
#    version: 2
#    # assuming the following information is same  as swav and dc2
#    n_negatives: 8192
#    n_classes: 3000
#    is_stopgrad: true
#    is_ema: false
#  model:
#    pred_dim: 128
#    projection_hid_width: 2048
#    projection_hid_depth: 1
#    architecture_exact: resnet50
#    architecture: resnet50
#    family: resnet
#    patch_size: null
#    projection_arch: mlp
#    n_parameters: 23508032
#    projection_nparameters: 5e6
#  representation:
#    z_dim: 2048
#    z_layer: avgpool
#  optimization:
#    epochs: 400
#    batch_size: 4096
#    optimizer: lars # larc
#    learning_rate: 0.3
#    weight_decay: 0.000001
#    scheduler: cosine
#  data:
#    pretraining_data: ImageNet-1K
#    finetuning_data: null
#    img_size: 224
#    views: 2x160+4x96
#    is_aug_invariant: true
#    augmentations:
#      - crop
#      - color
#      - blur
#      - gray
#      - flip
#      - normalize
#  metrics:
#    top1acc_in1k_official: 71.8
#    top1acc_in1k-1%_official: null
#    top1acc_in1k-c5_official: null
#  meta:
#    where: https://github.com/facebookresearch/swav
#    notes: null
#    month: 6
#    year: 2020
#    license: MIT
#    is_official: true
#    is_industry: true
#  compute:
#    n_pus: ???
#    pu_type: V100
#    time_hours: ???

selav2_rn50_ep400_2x224:
  ssl:
    objective: sela
    ssl_mode: clustering
    version: 2
    # assuming the following information is same  as swav and dc2
    n_negatives: 8192
    n_classes: 3000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 4096
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 67.2
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/swav
    notes: null
    month: 6
    year: 2020
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: ???
    pu_type: V100
    time_hours: ???

dc2_rn50_ep800_2x224_6x96:
  ssl:
    objective: deepcluster
    ssl_mode: clustering
    version: 2
    n_negatives: 32768
    n_classes: 3000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 800
    batch_size: 4096
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+6x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 75.2
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/swav
    notes: null
    month: 6
    year: 2020
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 64
    pu_type: V100
    time_hours: ???

dc2_rn50_ep400_2x160_4x96:
  ssl:
    objective: deepcluster
    ssl_mode: clustering
    version: 2
    n_negatives: 8192
    n_classes: 3000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 4096
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x260+4x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 74.3
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/swav
    notes: null
    month: 6
    year: 2020
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 64
    pu_type: V100
    time_hours: 21.4

dc2_rn50_ep400_2x224:
  ssl:
    objective: deepcluster
    ssl_mode: clustering
    version: 2
    n_negatives: 8192
    n_classes: 3000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 4096
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 70.2
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/swav
    notes: null
    month: 6
    year: 2020
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 64
    pu_type: V100
    time_hours: ???

swav_rn50w4:
  ssl:
    objective: swav
    ssl_mode: clustering
    version: 1
    n_negatives: 7680
    n_classes: 3000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 8192
    projection_hid_depth: 1
    architecture_exact: resnet50w4
    architecture: resnet50w4
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 375378176
    projection_nparameters: 68e6
  representation:
    z_dim: 8192
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 2560
    optimizer: lars # larc
    learning_rate: 0.48
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+6x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 77.3
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/swav
    notes: null
    month: 2
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 64
    pu_type: V100
    time_hours: ???

swav_rn50w2:
  ssl:
    objective: swav
    ssl_mode: clustering
    version: 1
    n_negatives: 8192
    n_classes: 3000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 8192
    projection_hid_depth: 1
    architecture_exact: resnet50w2
    architecture: resnet50w2
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 93907072
    projection_nparameters: 35e6
  representation:
    z_dim: 4096
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 4096
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+6x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 77.3
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/swav
    notes: null
    month: 2
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 128
    pu_type: V100
    time_hours: ???

swav_rn50_ep400_bs256:
  ssl:
    objective: swav
    ssl_mode: clustering
    version: 1
    n_negatives: 4352
    n_classes: 3000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 256
    optimizer: lars # larc
    learning_rate: 0.6
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+6x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 74.3
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/swav
    notes: null
    month: 2
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 4
    pu_type: V100
    time_hours: 348

swav_rn50_ep400_2x224:
  ssl:
    objective: swav
    ssl_mode: clustering
    version: 1
    n_negatives: 8192
    n_classes: 3000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 4096
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 70.1
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/swav
    notes: null
    month: 2
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 64
    pu_type: V100
    time_hours: 16.5

swav_rn50_ep400:
  ssl:
    objective: swav
    ssl_mode: clustering
    version: 1
    n_negatives: 8192
    n_classes: 3000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 4096
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+6x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 74.6
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/swav
    notes: null
    month: 2
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 64
    pu_type: V100
    time_hours: 25

swav_rn50_ep200_bs256:
  ssl:
    objective: swav
    ssl_mode: clustering
    version: 1
    n_negatives: 4352
    n_classes: 3000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 256
    optimizer: lars # larc
    learning_rate: 0.6
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+6x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 72.7
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/swav
    notes: null
    month: 2
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 4
    pu_type: V100
    time_hours: 348

swav_rn50_ep200:
  ssl:
    objective: swav
    ssl_mode: clustering
    version: 1
    n_negatives: 8192
    n_classes: 3000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 4096
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+6x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 73.9
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/swav
    notes: null
    month: 2
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 64
    pu_type: V100
    time_hours: 12.5

swav_rn50_ep100:
  ssl:
    objective: swav
    ssl_mode: clustering
    version: 1
    n_negatives: 8192
    n_classes: 3000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 4096
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+6x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 72.1
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/swav
    notes: null
    month: 2
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 64
    pu_type: V100
    time_hours: 6.25

swav_rn50:
  ssl:
    objective: swav
    ssl_mode: clustering
    version: 1
    n_negatives: 8192
    n_classes: 3000
    is_stopgrad: true
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 800
    batch_size: 4096
    optimizer: lars # larc
    learning_rate: 0.3
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+6x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 75.3
    top1acc_in1k-1%_official: 53.9
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/swav
    notes: null
    month: 2
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 64
    pu_type: V100
    time_hours: 50

### VICREG ###
vicreg_rn50w2:
  ssl:
    objective: vicreg
    ssl_mode: siamese
    version: 1
    n_negatives: 4096
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 8192
    projection_hid_width: 8192
    projection_hid_depth: 2
    architecture_exact: resnet50w2
    architecture: resnet50w2
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 93907072
    projection_nparameters: 151e6
  representation:
    z_dim: 4096
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 2048
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - gray
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 75.5
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vicreg
    notes: null
    month: 1
    year: 2022
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 32
    pu_type: Tesla V100
    time_hours: ???

vicreg_rn50:
  ssl:
    objective: vicreg
    ssl_mode: siamese
    version: 1
    n_negatives: 4096
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 8192
    projection_hid_width: 8192
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 151e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 2048
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - gray
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 73.2
    top1acc_in1k-1%_official: 54.8
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vicreg
    notes: null
    month: 1
    year: 2022
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 32
    pu_type: Tesla V100
    time_hours: 110 # see table 14 in their paper

### VICREGL ###
vicregl_rn50_alpha09:
  ssl:
    objective: vicregl
    ssl_mode: hierarchical
    version: 1
    n_negatives: 4096
    n_classes: null
    is_stopgrad: false
    is_ema: false
    other: alpha 0.9
  model:
    pred_dim: 8192
    projection_hid_width: 8192
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 151e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 300
    batch_size: 2048
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - gray
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 71.2
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vicregl
    notes: alpha=0.9 which weights global and local loss (larger means more for global)
    month: 10
    year: 2022
    license: Creative Commons Attribution-NonCommercial 4.0 International License
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 32
    pu_type: Tesla V100
    time_hours: null

vicregl_rn50_alpha075:
  ssl:
    objective: vicregl
    ssl_mode: hierarchical
    version: 1
    n_negatives: 4096
    n_classes: null
    is_stopgrad: false
    is_ema: false
    other: alpha 0.75
  model:
    pred_dim: 8192
    projection_hid_width: 8192
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 151e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 300
    batch_size: 2048
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - gray
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 70.4
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vicregl
    notes: alpha=0.75 which weights global and local loss (larger means more for global)
    month: 10
    year: 2022
    license: Creative Commons Attribution-NonCommercial 4.0 International License
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 32
    pu_type: Tesla V100
    time_hours: null

vicregl_convnexts_alpha09:
  ssl:
    objective: vicregl
    ssl_mode: hierarchical
    version: 1
    n_negatives: 3072
    n_classes: null
    is_stopgrad: false
    is_ema: false
    other: alpha 0.9
  model:
    pred_dim: 8192
    projection_hid_width: 8192
    projection_hid_depth: 2
    architecture_exact: convnexts
    architecture: convnexts
    family: convnext
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 50e6
    projection_nparameters: 151e6
  representation:
    z_dim: 768
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 384
    optimizer: adamw
    learning_rate: 0.00075
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+6x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - gray
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 75.9
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vicregl
    notes: alpha=0.9 which weights global and local loss (larger means more for global)
    month: 10
    year: 2022
    license: Creative Commons Attribution-NonCommercial 4.0 International License
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: Tesla V100
    time_hours: null

vicregl_convnexts_alpha075:
  ssl:
    objective: vicregl
    ssl_mode: hierarchical
    version: 1
    n_negatives: 3072
    n_classes: null
    is_stopgrad: false
    is_ema: false
    other: alpha 0.75
  model:
    pred_dim: 8192
    projection_hid_width: 8192
    projection_hid_depth: 2
    architecture_exact: convnexts
    architecture: convnexts
    family: convnext
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 50e6
    projection_nparameters: 151e6
  representation:
    z_dim: 768
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 384
    optimizer: adamw
    learning_rate: 0.00075
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+6x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - gray
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 74.6
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vicregl
    notes: alpha=0.75 which weights global and local loss (larger means more for global)
    month: 10
    year: 2022
    license: Creative Commons Attribution-NonCommercial 4.0 International License
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: Tesla V100
    time_hours: null

vicregl_convnextb_alpha09:
  ssl:
    objective: vicregl
    ssl_mode: hierarchical
    version: 1
    n_negatives: 4608
    n_classes: null
    is_stopgrad: false
    is_ema: false
    other: alpha 0.9
  model:
    pred_dim: 8192
    projection_hid_width: 8192
    projection_hid_depth: 2
    architecture_exact: convnextb
    architecture: convnextb
    family: convnext
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 85e6
    projection_nparameters: 153e6
  representation:
    z_dim: 1024
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 576
    optimizer: adamw
    learning_rate: 0.0005
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+6x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - gray
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 77.1
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vicregl
    notes: alpha=0.9 which weights global and local loss (larger means more for global)
    month: 10
    year: 2022
    license: Creative Commons Attribution-NonCommercial 4.0 International License
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: Tesla V100
    time_hours: null

vicregl_convnextb_alpha075:
  ssl:
    objective: vicregl
    ssl_mode: hierarchical
    version: 1
    n_negatives: 4608
    n_classes: null
    is_stopgrad: false
    is_ema: false
    other: alpha 0.75
  model:
    pred_dim: 8192
    projection_hid_width: 8192
    projection_hid_depth: 2
    architecture_exact: convnextb
    architecture: convnextb
    family: convnext
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 85e6
    projection_nparameters: 153e6
  representation:
    z_dim: 1024
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 576
    optimizer: adamw
    learning_rate: 0.0005
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224+6x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - gray
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 76.3
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vicregl
    notes: alpha=0.75 which weights global and local loss (larger means more for global)
    month: 10
    year: 2022
    license: Creative Commons Attribution-NonCommercial 4.0 International License
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: Tesla V100
    time_hours: null


vicregl_convnextxl_alpha075:
  ssl:
    objective: vicregl
    ssl_mode: hierarchical
    version: 1
    n_negatives: 7680
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 8192
    projection_hid_width: 8192
    projection_hid_depth: 2
    architecture_exact: convnextxl
    architecture: convnextxl
    family: convnext
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 350e6
    projection_nparameters: 161e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 150
    batch_size: 960
    optimizer: adamw
    learning_rate: 5e-05
    weight_decay: 1e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-22K
    finetuning_data: null
    img_size: 224
    views: 2x224+6x96
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - gray
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 79.4
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vicregl
    notes: not clear if alpha is 0.5 or 0.75 (log say the former, while table say the latter)
    month: 10
    year: 2022
    license: Creative Commons Attribution-NonCommercial 4.0 International License
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: Tesla V100
    time_hours: null

### VISSL ###

pirl_rn50w2_headMLP:
  ssl:
    objective: pirl
    ssl_mode: contrastive
    version: 1
    n_negatives: 32000
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50w2
    architecture: resnet50w2
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 93907072
    projection_nparameters: 17e6
  representation:
    z_dim: 4096
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 1024
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 0.0001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - solarize
      - photometric
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 70.9
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/projects/PIRL/README.md
    notes: null
    month: 12
    year: 2019
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 32
    pu_type: ???
    time_hours: ???

pirl_rn50w2:
  ssl:
    objective: pirl
    ssl_mode: contrastive
    version: 1
    n_negatives: 32000
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50w2
    architecture: resnet50w2
    family: resnet
    patch_size: null
    projection1_arch: linear
    projection2_arch: linear
    projection_same: true
    n_parameters: 93907072
    projection_nparameters: 0.6e6
  representation:
    z_dim: 4096
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 1024
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 0.0001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - solarize
      - photometric
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 69.3
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/projects/PIRL/README.md
    notes: null
    month: 12
    year: 2019
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 32
    pu_type: ???
    time_hours: ???

pirl_rn50_ep200_headMLP:
  ssl:
    objective: pirl
    ssl_mode: contrastive
    version: 1
    n_negatives: 32000
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 1024
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 0.0001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - solarize
      - photometric
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 65.8
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/projects/PIRL/README.md
    notes: null
    month: 12
    year: 2019
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 32
    pu_type: ???
    time_hours: ???

pirl_rn50_headMLP:
  ssl:
    objective: pirl
    ssl_mode: contrastive
    version: 1
    n_negatives: 32000
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 5e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 800
    batch_size: 1024
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 0.0001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - solarize
      - photometric
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 69.9
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/projects/PIRL/README.md
    notes: null
    month: 12
    year: 2019
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 32
    pu_type: ???
    time_hours: ???

pirl_rn50_ep200:
  ssl:
    objective: pirl
    ssl_mode: contrastive
    version: 1
    n_negatives: 32000
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: linear
    projection2_arch: linear
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 0.4e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 1024
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 0.0001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - solarize
      - photometric
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 62.9
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/projects/PIRL/README.md
    notes: null
    month: 12
    year: 2019
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 32
    pu_type: ???
    time_hours: ???

pirl_rn50:
  ssl:
    objective: pirl
    ssl_mode: contrastive
    version: 1
    n_negatives: 32000
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: linear
    projection2_arch: linear
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 0.4e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 800
    batch_size: 1024
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 0.0001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - solarize
      - photometric
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 64.29
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/projects/PIRL/README.md
    notes: null
    month: 12
    year: 2019
    license: MIT
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 32
    pu_type: ???
    time_hours: ???

#npidpp_rn50w2:
#  ssl:
#    objective: npid
#    ssl_mode: contrastive
#    version: 2 #  npid++ is an improvement of npid
#    n_negatives: 32000
#    n_classes: null
#    is_stopgrad: false
#    is_ema: false
#  model:
#    pred_dim: 128
#    projection_hid_width: null
#    projection_hid_depth: null
#    architecture_exact: resnet50w2
#    architecture: resnet50w2
#    family: resnet
#    patch_size: null
#    projection_arch: linear
#    n_parameters: 93907072
#    projection_nparameters: 0.5e6
#  representation:
#    z_dim: 4096
#    z_layer: avgpool
#  optimization:
#    epochs: 800
#    batch_size: 1024
#    optimizer: sgd
#    learning_rate: 0.03
#    weight_decay: 0.0001
#    scheduler: cosine
#  data:
#    pretraining_data: ImageNet-1K
#    finetuning_data: null
#    img_size: 224
#    views: 2x224
#    is_aug_invariant: true
#    augmentations:
#      - crop
#      - color
#      - flip
#      - normalize
#  metrics:
#    top1acc_in1k_official: 62.73
#  meta:
#    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
#    notes: null
#    month: 12
#    year: 2019
#    license: MIT
#    is_official: false
#  compute:
#    n_pus: 32
#    pu_type: ???
#    time_hours: ???

npidpp_rn50:
  ssl:
    objective: npid
    ssl_mode: contrastive
    version: 2 #  npid++ is an improvement of npid
    n_negatives: 32000
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: linear
    projection2_arch: linear
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 0.3e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 800
    batch_size: 1024
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 0.0001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 56.68
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 12
    year: 2019
    license: MIT
    is_official: false
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 32
    pu_type: ???
    time_hours: ???

npid_rn50:
  ssl:
    objective: npid
    ssl_mode: contrastive
    version: 1
    n_negatives: 4096
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: linear
    projection2_arch: linear
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 0.3e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 256
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 0.0001
    scheduler: multistep
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 52.73
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 5
    year: 2018
    license: MIT
    is_official: false
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???

clusterfit_rn50:
  ssl:
    objective: clusterfit
    ssl_mode: clustering
    version: 1
    n_negatives: null
    n_classes: 16000
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: linear
    projection2_arch: null
    projection_same: null
    n_parameters: 23508032
    projection_nparameters: 33e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 105
    batch_size: 256
    optimizer: sgd
    learning_rate: 0.1
    weight_decay: 0.0001
    scheduler: multistep
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 53.63
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 12
    year: 2019
    license: MIT
    is_official: false
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???

jigsaw_rn50:
  ssl:
    objective: jigsaw
    ssl_mode: transform
    version: 1
    n_negatives: null
    n_classes: 2000
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: 9000
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: null
    projection_same: null
    n_parameters: 23508032
    projection_nparameters: 20e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 105
    batch_size: 256
    optimizer: sgd
    learning_rate: 0.1
    weight_decay: 0.0001
    scheduler: multistep
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 224
    is_aug_invariant: true
    augmentations:
      - crop
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 46.58
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 3
    year: 2016
    license: MIT
    is_official: false
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???

jigsaw_rn50_in22k:
  ssl:
    objective: jigsaw
    ssl_mode: transform
    version: 1
    n_negatives: null
    n_classes: 2000
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: 9000
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: null
    projection_same: null
    n_parameters: 23508032
    projection_nparameters: 20e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 105
    batch_size: 256
    optimizer: sgd
    learning_rate: 0.1
    weight_decay: 0.0001
    scheduler: multistep
  data:
    pretraining_data: ImageNet-22K
    finetuning_data: null
    img_size: 224
    views: 224
    is_aug_invariant: true
    augmentations:
      - crop
      - gray
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 53.09
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 3
    year: 2016
    license: MIT
    is_official: false
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???

simclr_rn101_ep100:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 8192
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet101
    architecture: resnet101
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 42500160
    projection_nparameters: 4456448
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 4096
    optimizer: lars #larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 62.76
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 2
    year: 2020
    license: MIT
    is_official: false
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 64
    pu_type: ???
    time_hours: ???

simclr_rn101:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 8192
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet101
    architecture: resnet101
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 42500160
    projection_nparameters: 68e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 4096
    optimizer: lars #larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 71.56
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 2
    year: 2020
    license: MIT
    is_official: false
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 64
    pu_type: ???
    time_hours: ???

simclr_rn50w4:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 8192
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 8192
    projection_hid_depth: 1
    architecture_exact: resnet50w4
    architecture: resnet50w4
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 375378176
    projection_nparameters: 68e6
  representation:
    z_dim: 8192
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 4096
    optimizer: lars #larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 71.61
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 2
    year: 2020
    license: MIT
    is_official: false
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 64
    pu_type: ???
    time_hours: ???

simclr_rn50w2_ep100:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 8192
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50w2
    architecture: resnet50w2
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 93907072
    projection_nparameters: 17e6
  representation:
    z_dim: 4096
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 4096
    optimizer: lars #larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 69.82
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 2
    year: 2020
    license: MIT
    is_official: false
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 64
    pu_type: ???
    time_hours: ???

simclr_rn50w2:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 8192
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50w2
    architecture: resnet50w2
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 93907072
    projection_nparameters: 17e6
  representation:
    z_dim: 4096
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 4096
    optimizer: lars #larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 73.84
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 2
    year: 2020
    license: MIT
    is_official: false
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 64
    pu_type: ???
    time_hours: ???

simclr_rn50_bs4096_ep100:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 8192
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 4456448
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 100
    batch_size: 4096
    optimizer: lars #larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 64.4
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 2
    year: 2020
    license: MIT
    is_official: false
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 64
    pu_type: ???
    time_hours: ???

simclr_rn50_ep800:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 8192
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 4456448
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 800
    batch_size: 4096
    optimizer: lars #larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 69.68
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 2
    year: 2020
    license: MIT
    is_official: false
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 64
    pu_type: ???
    time_hours: ???

simclr_rn50_ep400:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 8192
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 4456448
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 400
    batch_size: 4096
    optimizer: lars #larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 67.71
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 2
    year: 2020
    license: MIT
    is_official: false
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 64
    pu_type: ???
    time_hours: ???

simclr_rn50_ep200:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 8192
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 4456448
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 4096
    optimizer: lars #larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 66.61
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 2
    year: 2020
    license: MIT
    is_official: false
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 64
    pu_type: ???
    time_hours: ???

simclr_rn50:
  ssl:
    objective: simclr
    ssl_mode: contrastive
    version: 1
    n_negatives: 8192
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 4456448
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 4096
    optimizer: lars #larc
    learning_rate: 0.3
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - blur
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 68.8
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 2
    year: 2020
    license: MIT
    is_official: false
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 64
    pu_type: ???
    time_hours: ???

rotnet_rn50_in22k:
  ssl:
    objective: rotnet
    ssl_mode: transform # predict the transformation
    version: 1
    n_negatives: null
    n_classes: 4
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: linear
    projection2_arch: null
    projection_same: null
    n_parameters: 23508032
    projection_nparameters: 0.1e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 105
    batch_size: 256
    optimizer: sgd
    learning_rate: 0.1
    weight_decay: 0.0001
    scheduler: multistep
  data:
    pretraining_data: ImageNet-22K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 54.89
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 3
    year: 2018
    license: MIT
    is_official: false
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???

rotnet_rn50_in1k:
  ssl:
    objective: rotnet
    ssl_mode: transform # predict the transformation
    version: 1
    n_negatives: null
    n_classes: 4
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: null
    projection_hid_width: null
    projection_hid_depth: null
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: linear
    projection2_arch: null
    projection_same: null
    n_parameters: 23508032
    projection_nparameters: 0.1e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 105
    batch_size: 256
    optimizer: sgd
    learning_rate: 0.1
    weight_decay: 0.0001
    scheduler: multistep
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - normalize
  metrics:
    top1acc_in1k_official: 48.2
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 3
    year: 2018
    license: MIT
    is_official: false
    is_industry: false
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???

mocov2_rn50_vissl:
  ssl:
    objective: moco
    ssl_mode: contrastive
    version: 2
    n_negatives: 65536
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 128
    projection_hid_width: 2048
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 4456448
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 200
    batch_size: 256
    optimizer: sgd
    learning_rate: 0.03
    weight_decay: 0.0001
    scheduler: multistep
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - gray
      - blur
      - normalize
  metrics:
    top1acc_in1k_official: 66.4
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 9
    year: 2020
    license: MIT
    is_official: false
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 8
    pu_type: ???
    time_hours: ???

barlow_rn50_ep300:
  ssl:
    objective: barlow_twins
    ssl_mode: siamese
    version: 1
    n_negatives: 4095
    n_classes: null
    is_stopgrad: false
    is_ema: false
  model:
    pred_dim: 8192
    projection_hid_width: 8192
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 151e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 300
    batch_size: 2048
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1.5e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - gray
      - blur
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 70.75
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 3
    year: 2021
    license: MIT
    is_official: false
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 32
    pu_type: ???
    time_hours: ???

barlow_rn50:
  ssl:
    objective: barlow_twins
    ssl_mode: siamese
    version: 1
    n_negatives: 4095
    n_classes: null
    is_stopgrad: false
    is_ema: False
  model:
    pred_dim: 8192
    projection_hid_width: 8192
    projection_hid_depth: 2
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: true
    n_parameters: 23508032
    projection_nparameters: 151e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 2048
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 0.000001
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - gray
      - blur
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 71.8
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/facebookresearch/vissl/blob/main/MODEL_ZOO.md
    notes: null
    month: 3
    year: 2021
    license: MIT
    is_official: false
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 32
    pu_type: ???
    time_hours: ???

### BYOL ###
byol_rn50_bs4096:
  ssl:
    objective: byol
    ssl_mode: siamese
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: false
    n_parameters: 23508032
    projection_nparameters: 12e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 4096
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1.5e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - gray
      - blur
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 74.4
    top1acc_in1k-1%_official: 53.2
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/deepmind/deepmind-research/tree/master/byol
    notes: the weights are from the official implementation but converted tf -> torch. The main difference in the resnet50 is how padding is done.
    month: 6
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 512
    pu_type: Cloud TPU v3
    time_hours: 8

byol_rn50_bs2048:
  ssl:
    objective: byol
    ssl_mode: siamese
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: false
    n_parameters: 23508032
    projection_nparameters: 12e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 2048
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1.5e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - gray
      - blur
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 72.4
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/deepmind/deepmind-research/tree/master/byol
    notes: the weights are from the official implementation but converted tf -> torch. The main difference in the resnet50 is how padding is done.
    month: 6
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 512
    pu_type: Cloud TPU v3
    time_hours: ???

byol_rn50_bs1024:
  ssl:
    objective: byol
    ssl_mode: siamese
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: false
    n_parameters: 23508032
    projection_nparameters: 12e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 1024
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1.5e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - gray
      - blur
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 72.2
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/deepmind/deepmind-research/tree/master/byol
    notes: the weights are from the official implementation but converted tf -> torch. The main difference in the resnet50 is how padding is done.
    month: 6
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 512
    pu_type: Cloud TPU v3
    time_hours: ???

byol_rn50_bs512:
  ssl:
    objective: byol
    ssl_mode: siamese
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: false
    n_parameters: 23508032
    projection_nparameters: 12e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 512
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1.5e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - gray
      - blur
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 72.2
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/deepmind/deepmind-research/tree/master/byol
    notes: the weights are from the official implementation but converted tf -> torch. The main difference in the resnet50 is how padding is done.
    month: 6
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 512
    pu_type: Cloud TPU v3
    time_hours: ???

byol_rn50_bs256:
  ssl:
    objective: byol
    ssl_mode: siamese
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: false
    n_parameters: 23508032
    projection_nparameters: 12e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 256
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1.5e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - gray
      - blur
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 71.8
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/deepmind/deepmind-research/tree/master/byol
    notes: the weights are from the official implementation but converted tf -> torch. The main difference in the resnet50 is how padding is done.
    month: 6
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 512
    pu_type: Cloud TPU v3
    time_hours: ???

byol_rn50_bs128:
  ssl:
    objective: byol
    ssl_mode: siamese
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: false
    n_parameters: 23508032
    projection_nparameters: 12e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 128
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1.5e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - gray
      - blur
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 69.6
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/deepmind/deepmind-research/tree/master/byol
    notes: the weights are from the official implementation but converted tf -> torch. The main difference in the resnet50 is how padding is done.
    month: 6
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 512
    pu_type: Cloud TPU v3
    time_hours: ???

byol_rn50_bs64:
  ssl:
    objective: byol
    ssl_mode: siamese
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: false
    n_parameters: 23508032
    projection_nparameters: 12e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 64
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1.5e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - gray
      - blur
      - solarize
      - normalize
  metrics:
    top1acc_in1k_official: 59.7
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/deepmind/deepmind-research/tree/master/byol
    notes: the weights are from the official implementation but converted tf -> torch. The main difference in the resnet50 is how padding is done.
    month: 6
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 512
    pu_type: Cloud TPU v3
    time_hours: ???

byol_rn50_augCropBlur:
  ssl:
    objective: byol
    ssl_mode: siamese
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: false
    n_parameters: 23508032
    projection_nparameters: 12e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 4096
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1.5e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - blur
      - normalize
  metrics:
    top1acc_in1k_official: 61.1
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/deepmind/deepmind-research/tree/master/byol
    notes: the weights are from the official implementation but converted tf -> torch. The main difference in the resnet50 is how padding is done.
    month: 6
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 512
    pu_type: Cloud TPU v3
    time_hours: ???

byol_rn50_augCropColor:
  ssl:
    objective: byol
    ssl_mode: siamese
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: false
    n_parameters: 23508032
    projection_nparameters: 12e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 4096
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1.5e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - color
      - solarize # I'm assuming that solarize is considered as color (couldn't find inofmration)
      - normalize
  metrics:
    top1acc_in1k_official: 70.7
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/deepmind/deepmind-research/tree/master/byol
    notes: the weights are from the official implementation but converted tf -> torch. The main difference in the resnet50 is how padding is done.
    month: 6
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 512
    pu_type: Cloud TPU v3
    time_hours: ???

byol_rn50_augCrop:
  ssl:
    objective: byol
    ssl_mode: siamese
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: false
    n_parameters: 23508032
    projection_nparameters: 12e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 4096
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1.5e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - normalize
  metrics:
    top1acc_in1k_official: 59.4
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/deepmind/deepmind-research/tree/master/byol
    notes: the weights are from the official implementation but converted tf -> torch. The main difference in the resnet50 is how padding is done.
    month: 6
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 512
    pu_type: Cloud TPU v3
    time_hours: ???

byol_rn50_augNocolor:
  ssl:
    objective: byol
    ssl_mode: siamese
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: false
    n_parameters: 23508032
    projection_nparameters: 12e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 4096
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1.5e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - gray
      - blur
      - normalize
  metrics:
    top1acc_in1k_official: 59.4
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/deepmind/deepmind-research/tree/master/byol
    notes: the weights are from the official implementation but converted tf -> torch. The main difference in the resnet50 is how padding is done.
    month: 6
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 512
    pu_type: Cloud TPU v3
    time_hours: ???

byol_rn50_augNogray:
  ssl:
    objective: byol
    ssl_mode: siamese
    version: 1
    n_negatives: null
    n_classes: null
    is_stopgrad: true
    is_ema: true
  model:
    pred_dim: 256
    projection_hid_width: 4096
    projection_hid_depth: 1
    architecture_exact: resnet50
    architecture: resnet50
    family: resnet
    patch_size: null
    projection1_arch: mlp
    projection2_arch: mlp
    projection_same: false
    n_parameters: 23508032
    projection_nparameters: 12e6
  representation:
    z_dim: 2048
    z_layer: avgpool
  optimization:
    epochs: 1000
    batch_size: 4096
    optimizer: lars
    learning_rate: 0.2
    weight_decay: 1.5e-6
    scheduler: cosine
  data:
    pretraining_data: ImageNet-1K
    finetuning_data: null
    img_size: 224
    views: 2x224
    is_aug_invariant: true
    augmentations:
      - crop
      - flip
      - color
      - solarize
      - blur
      - normalize
  metrics:
    top1acc_in1k_official: 70.3
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/deepmind/deepmind-research/tree/master/byol
    notes: the weights are from the official implementation but converted tf -> torch. The main difference in the resnet50 is how padding is done.
    month: 6
    year: 2020
    license: CC BY-NC 4.0
    is_official: true
    is_industry: true
    is_our_extract: false
  compute:
    n_pus: 512
    pu_type: Cloud TPU v3
    time_hours: ???

######################## HERE ARE ALL SUPERVISED MODELS ########################
### TIMM ###
sup_vitS16_dino_extractB:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: vits16 cls+avg
    architecture: vits
    patch_size: 16
  representation:
    z_dim: 768
    z_layer: cls+avg
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

sup_vitS16_dino:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: vits16 4xcls
    architecture: vits
    patch_size: 16
  representation:
    z_dim: 1536
    z_layer: 4xcls
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

sup_vitS16:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: vits16 cls
    architecture: vits
    patch_size: 16
  representation:
    z_dim: 384
    z_layer: cls
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: 81.396
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

sup_vitL16:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: vitl16 cls
    architecture: vitl
    patch_size: 16
  representation:
    z_dim: 1024
    z_layer: cls
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: 85.844
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

sup_vitB32:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: vitb32 cls
    architecture: vitb
    patch_size: 32
  representation:
    z_dim: 768
    z_layer: cls
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: 80.724
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

sup_vitB16_dino_extractS:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: vitb16 4xcls
    architecture: vitb
    patch_size: 16
  representation:
    z_dim: 3072
    z_layer: 4xcls
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

sup_vitB16_dino:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: vitb16 cls+avg
    architecture: vitb
    patch_size: 16
  representation:
    z_dim: 1536
    z_layer: cls+avg
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

sup_vitB16:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: vitb16 cls
    architecture: vitb
    patch_size: 16
  representation:
    z_dim: 768
    z_layer: cls
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: 84.530
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

sup_vitB8_dino:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: vitb8 cls+avg
    architecture: vitb
    patch_size: 8
  representation:
    z_dim: 1536
    z_layer: cls+avg
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

sup_vitB8:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: vitb8 cls
    architecture: vitb
    patch_size: 8
  representation:
    z_dim: 768
    z_layer: cls
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: 85.790
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

sup_convnextS:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: convnexts
    architecture: convnexts
    patch_size: null
  representation:
    z_dim: 768
    z_layer: avgpool
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: 83.1
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: true # weights are official

sup_convnextB:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: convnextb
    architecture: convnextb
    patch_size: null
  representation:
    z_dim: 1024
    z_layer: avgpool
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: 83.8
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: true # weights are official

### Torchvision ###
sup_rn50:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: resnet50
    architecture: resnet50
    patch_size: null
  representation:
    z_dim: 2048
    z_layer: avgpool
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: 76.13
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/pytorch/vision
    license: BSD 3-Clause
    is_official: false

sup_rn50w2:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: wide_resnet50_2
    architecture: resnet50w2
    patch_size: null
  representation:
    z_dim: 2048
    z_layer: avgpool
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: 78.468
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/pytorch/vision
    license: BSD 3-Clause
    is_official: false
    notes: this is not exactly correct, we are using wide_resnet50_2 (Wide_ResNet101_2_Weights.IMAGENET1K_V2) instead of resnet50w2 because I couldn't find parameters for the latter

sup_rn101:
  ssl:
    ssl_mode: supervised
  model:
    architecture_exact: resnet101
    architecture: resnet101
    patch_size: null
  representation:
    z_dim: 2048
    z_layer: avgpool
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: 78.312
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/pytorch/vision
    license: BSD 3-Clause
    is_official: false


###################################### ALL UNTRAINED MODELS ######################################
init_vitS16_dino_extractB:
  ssl:
    ssl_mode: initialized
  model:
    architecture_exact: vits16 cls+avg
    architecture: vits
    patch_size: 16
    family: vit
    patch_size: 16
  representation:
    z_dim: 768
    z_layer: cls+avg
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

init_vitS16_dino:
  ssl:
    ssl_mode: initialized
  model:
    architecture_exact: vits16 4xcls
    architecture: vits
    patch_size: 16
    family: vit
    patch_size: 16
  representation:
    z_dim: 1536
    z_layer: 4xcls
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

init_vitS16:
  ssl:
    ssl_mode: initialized
  model:
    architecture_exact: vits16 cls
    architecture: vits
    patch_size: 16
    family: vit
  representation:
    z_dim: 384
    z_layer: cls
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

init_vitL16:
  ssl:
    ssl_mode: initialized
  model:
    architecture_exact: vitl16 cls
    architecture: vitl
    family: vit
    patch_size: 16
  representation:
    z_dim: 1024
    z_layer: cls
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

init_vitB32:
  ssl:
    ssl_mode: initialized
  model:
    architecture_exact: vitb32 cls
    architecture: vitb
    family: vit
    patch_size: 32
  representation:
    z_dim: 768
    z_layer: cls
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

init_vitB16_dino_extractS:
  ssl:
    ssl_mode: initialized
  model:
    architecture_exact: vitb16 4xcls
    architecture: vitb
    family: vit
    patch_size: 16
  representation:
    z_dim: 3072
    z_layer: 4xcls
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

init_vitB16_dino:
  ssl:
    ssl_mode: initialized
  model:
    architecture_exact: vitb16 cls+avg
    architecture: vitb
    family: vit
    patch_size: 16
  representation:
    z_dim: 1536
    z_layer: cls+avg
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

init_vitB16:
  ssl:
    ssl_mode: initialized
  model:
    architecture_exact: vitb16 cls
    architecture: vitb
    family: vit
    patch_size: 16
  representation:
    z_dim: 768
    z_layer: cls
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

init_vitB8_dino:
  ssl:
    ssl_mode: initialized
  model:
    architecture_exact: vitb8 cls+avg
    architecture: vitb
    family: vit
    patch_size: 8
  representation:
    z_dim: 1536
    z_layer: cls+avg
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

init_vitB8:
  ssl:
    ssl_mode: initialized
  model:
    architecture_exact: vitb8 cls
    architecture: vitb
    family: vit
    patch_size: 8
  representation:
    z_dim: 768
    z_layer: cls
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/rwightman/pytorch-image-models
    license: Apache 2.0
    is_official: false

init_rn50:
  ssl:
    ssl_mode: initialized
  model:
    architecture_exact: resnet50
    architecture: resnet50
    patch_size: null
    family: resnet
  representation:
    z_dim: 2048
    z_layer: avgpool
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/pytorch/vision
    license: BSD 3-Clause
    is_official: false

init_rn50_d4096:
  ssl:
    ssl_mode: initialized
  model:
    architecture_exact: resnet50d4096
    architecture: resnet50
    patch_size: null
    family: resnet
  representation:
    z_dim: 4096
    z_layer: avgpool
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/pytorch/vision
    license: BSD 3-Clause
    is_official: false

init_rn50_d512:
  ssl:
    ssl_mode: initialized
  model:
    architecture_exact: resnet50d512
    architecture: resnet50
    patch_size: null
    family: resnet
  representation:
    z_dim: 512
    z_layer: avgpool
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/pytorch/vision
    license: BSD 3-Clause
    is_official: false

init_rn50_d1024:
  ssl:
    ssl_mode: initialized
  model:
    architecture_exact: resnet50d1024
    architecture: resnet50
    patch_size: null
    family: resnet
  representation:
    z_dim: 1024
    z_layer: avgpool
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/pytorch/vision
    license: BSD 3-Clause
    is_official: false

init_rn50w2:
  ssl:
    ssl_mode: initialized
  model:
    architecture_exact: wide_resnet50_2
    architecture: resnet50w2
    patch_size: null
    family: resnet
  representation:
    z_dim: 2048
    z_layer: avgpool
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/pytorch/vision
    license: BSD 3-Clause
    is_official: false
    notes: this is not exactly correct, we are using wide_resnet50_2 (Wide_ResNet101_2_Weights.IMAGENET1K_V2) instead of resnet50w2 because I couldn't find parameters for the latter

init_rn101:
  ssl:
    ssl_mode: initialized
  model:
    architecture_exact: resnet101
    architecture: resnet101
    patch_size: null
    family: resnet
  representation:
    z_dim: 2048
    z_layer: avgpool
  data:
    img_size: 224
  metrics:
    top1acc_in1k_official: null
    top1acc_in1k-1%_official: null
    top1acc_in1k-c5_official: null
  meta:
    where: https://github.com/pytorch/vision
    license: BSD 3-Clause
    is_official: false
